
# python recommedation system using transformer and give data set

# import pandas as pd
# from surprise import Dataset
# from surprise.model_selection import train_test_split

# # Load the MovieLens dataset
# data = Dataset.load_builtin('ml-100k')
# trainset, testset = train_test_split(data, test_size=0.2)

# # Convert the trainset to a pandas dataframe for easier handling
# train_df = pd.DataFrame(trainset.build_testset(), columns=['userId', 'movieId', 'rating'])
# test_df = pd.DataFrame(testset, columns=['userId', 'movieId', 'rating'])


# from sklearn.preprocessing import LabelEncoder

# # Encode user and item ids
# user_encoder = LabelEncoder()
# item_encoder = LabelEncoder()

# train_df['userId'] = user_encoder.fit_transform(train_df['userId'])
# train_df['movieId'] = item_encoder.fit_transform(train_df['movieId'])
# test_df['userId'] = user_encoder.transform(test_df['userId'])
# test_df['movieId'] = item_encoder.transform(test_df['movieId'])

# # Create interaction sequences
# interaction_sequences = train_df.groupby('userId')['movieId'].apply(list).reset_index()


# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, Dataset

# class TransformerRecommender(nn.Module):
#     def __init__(self, num_users, num_items, d_model, nhead, num_layers):
#         super(TransformerRecommender, self).__init__()
#         self.user_embedding = nn.Embedding(num_users, d_model)
#         self.item_embedding = nn.Embedding(num_items, d_model)
#         self.transformer = nn.Transformer(d_model, nhead, num_layers)
#         self.fc = nn.Linear(d_model, num_items)

#     def forward(self, user_ids, item_ids):
#         user_embeds = self.user_embedding(user_ids).unsqueeze(1)
#         item_embeds = self.item_embedding(item_ids)
#         transformer_output = self.transformer(item_embeds, user_embeds)
#         output = self.fc(transformer_output.squeeze(1))
#         return output

# # Define hyperparameters
# d_model = 64
# nhead = 4
# num_layers = 2
# num_users = train_df['userId'].nunique()
# num_items = train_df['movieId'].nunique()

# # Create the model
# model = TransformerRecommender(num_users, num_items, d_model, nhead, num_layers)


# class InteractionDataset(Dataset):
#     def __init__(self, interactions):
#         self.interactions = interactions

#     def __len__(self):
#         return len(self.interactions)

#     def __getitem__(self, idx):
#         return torch.tensor(self.interactions[idx], dtype=torch.long)

# # Create DataLoader for training
# train_loader = DataLoader(InteractionDataset(interaction_sequences['movieId'].tolist()), batch_size=32, shuffle=True)

# # Define loss and optimizer
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=0.001)

# # Training loop
# for epoch in range(10):
#     model.train()
#     total_loss = 0
#     for batch in train_loader:
#         optimizer.zero_grad()
#         user_ids = torch.arange(len(batch))
#         output = model(user_ids, batch)
#         loss = criterion(output, batch[:, 1:])
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#     print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')

# def recommend(model, user_id, top_k=10):
#     user_id = torch.tensor([user_id], dtype=torch.long)
#     item_ids = torch.arange(num_items).unsqueeze(0)
#     with torch.no_grad():
#         output = model(user_id, item_ids)
#         _, recommended_items = torch.topk(output, top_k)
#     return item_encoder.inverse_transform(recommended_items.numpy().flatten())

# # Example: Recommend top 10 movies for user 0
# recommended_movies = recommend(model, 0, top_k=10)
# print("Recommended Movies:", recommended_movies)

# pip install scikit-surprise
# not use surprise

import pandas as pd

# Download the MovieLens dataset
url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.data'
data = pd.read_csv(url, sep='\t', names=['userId', 'movieId', 'rating', 'timestamp'])

# Drop the timestamp column
data.drop('timestamp', axis=1, inplace=True)




# Define hyperparameters
d_model = 64
nhead = 4
num_layers = 2
num_users = data['userId'].nunique()
num_items = data['movieId'].nunique()
padding_idx = num_items  # Padding token index
from sklearn.preprocessing import LabelEncoder
import numpy as np
import torch

# Encode user and item ids
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

data['userId'] = user_encoder.fit_transform(data['userId'])
data['movieId'] = item_encoder.fit_transform(data['movieId'])

# Create interaction sequences
interaction_sequences = data.groupby('userId')['movieId'].apply(list).reset_index()

# Determine the maximum sequence length
max_seq_length = interaction_sequences['movieId'].apply(len).max()

# Pad sequences
def pad_sequence(seq, max_length):
    return seq + [num_items] * (max_length - len(seq))

interaction_sequences['movieId'] = interaction_sequences['movieId'].apply(lambda x: pad_sequence(x, max_seq_length))

# Convert to tensor
interaction_sequences_tensor = torch.tensor(interaction_sequences['movieId'].tolist(), dtype=torch.long)


from sklearn.preprocessing import LabelEncoder
import numpy as np
import torch

# Encode user and item ids
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()

data['userId'] = user_encoder.fit_transform(data['userId'])
data['movieId'] = item_encoder.fit_transform(data['movieId'])

# Create interaction sequences
interaction_sequences = data.groupby('userId')['movieId'].apply(list).reset_index()

# Determine the maximum sequence length
max_seq_length = interaction_sequences['movieId'].apply(len).max()

interaction_sequences['movieId'] = interaction_sequences['movieId'].apply(lambda x: pad_sequence(x, max_seq_length))

# Convert to tensor
interaction_sequences_tensor = torch.tensor(interaction_sequences['movieId'].tolist(), dtype=torch.long)
import torch
import torch.nn as nn
import torch.optim as optim

class TransformerRecommender(nn.Module):
    def __init__(self, num_users, num_items, d_model, nhead, num_layers, padding_idx):
        super(TransformerRecommender, self).__init__()
        self.user_embedding = nn.Embedding(num_users, d_model)
        self.item_embedding = nn.Embedding(num_items + 1, d_model, padding_idx=padding_idx)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, num_items)

    def forward(self, user_ids, item_ids):
        user_embeds = self.user_embedding(user_ids).unsqueeze(1)  # (batch_size, 1, d_model)
        user_embeds = user_embeds.repeat(1, item_ids.size(1), 1)  # (batch_size, seq_length, d_model)
        item_embeds = self.item_embedding(item_ids)  # (batch_size, seq_length, d_model)
        transformer_input = user_embeds + item_embeds  # Combine user and item embeddings
        transformer_output = self.transformer(transformer_input)  # (batch_size, seq_length, d_model)
        output = self.fc(transformer_output)  # (batch_size, seq_length, num_items)
        return output



# Create the model
model = TransformerRecommender(num_users, num_items, d_model, nhead, num_layers, padding_idx)
from torch.utils.data import DataLoader, Dataset

class InteractionDataset(Dataset):
    def __init__(self, interactions):
        self.interactions = interactions

    def __len__(self):
        return len(self.interactions)

    def __getitem__(self, idx):
        return self.interactions[idx]

# Create DataLoader for training
train_loader = DataLoader(InteractionDataset(interaction_sequences_tensor), batch_size=32, shuffle=True)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    model.train()
    total_loss = 0
    i=1
    for batch in train_loader:
        print('epoch',epoch,'i',i)
        optimizer.zero_grad()
        user_ids = torch.arange(len(batch))
        output = model(user_ids, batch[:, :-1])
        loss = criterion(output.view(-1, num_items), batch[:, 1:].contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        i+=1
    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')

def recommend(model, user_id, top_k=10):
    user_id = torch.tensor([user_id], dtype=torch.long)
    item_ids = torch.arange(num_items).unsqueeze(0)
    with torch.no_grad():
        output = model(user_id, item_ids)
        _, recommended_items = torch.topk(output[0], top_k)
    return item_encoder.inverse_transform(recommended_items.numpy().flatten())

# Example: Recommend top 10 movies for user 0
recommended_movies = recommend(model, 0, top_k=10)
print("Recommended Movies:", recommended_movies)
