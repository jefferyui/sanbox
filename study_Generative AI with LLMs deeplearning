Generative AI with LLMs deeplearning
introduction-week-1~2
transformer
本视频主要介绍了本周课程的主要内容，包括 Transformer 网络的工作原理、其架构中的多注意力机制等，还提到了 Transformer 在视觉等领域的应用


W1 3 Generative AI & LLMs
本视频介绍生成式人工智能及大语言模型，包括其使用场景、工作原理

W1 4 LLM use cases and tasks
介绍了大型语言模型（LLM）的多种使用案例和任务，包括文本生成中的不同任务如写文章、总结对话、翻译，信息检索中的命名实体识别，以及通过连接外部数据源增强 LLM 等

W1 5 Generating Text
循环神经网络（RNN）到 Transformer 架构带来生成式 AI 的重大变革

W1 6 Transformers Architecture
Transformer 架構的關鍵屬性自我注意力機制，以及 Transformer 架構的高層運作方式，如編碼器和解碼器、嵌入層、位置編碼、多頭自我注意力和全連接前饋網絡

W1 7 Generating Text with Transformers
编码器和解码器的作用、不同类型的 Transformer 模型（如编码器模型、编码器 - 解码器模型、解码器模型）

W1 8 Prompting and prompt engineering
本视频介绍了提示工程，包括提示、推理、完成等术语，重点讲解了通过在提示中加入示例进行上下文学习来改善模型输出，如零次推断、一次推断、少次推断等，还提到模型性能不佳时可尝试微调

W1 9 Generative configuration
本视频介绍了影响语言模型生成下一个词的方法和相关配置参数，包括最大生成词数、随机采样、Top K 和 Top P 采样技术以及温度参数对输出的影响，最后提到下一个视频将基于这些知识构建语言模型驱动的应用。

W1 10 Generative AI Project Lifecycle
本视频介绍了生成式 AI 项目生命周期，包括定义项目范围、选择模型训练方式、评估和改进模型性能、运用强化学习确保模型良好表现、部署优化模型以及考虑额外基础设施以克服语言模型的局限性。

W1 11 Introduction to AWS Lab
AWS 操作
W1 12 Lab 1 walkthrough

W1 13 Pre training large language models
提到了模型规模与能力的关系及训练大型模型的挑战

W1 14 Computational challenges of training LLMs

本视频主要介绍了训练大型语言模型时面临的内存挑战及量化这一解决方案，包括不同精度数据类型对内存占用的影响及 B float 16 的优势，还提到了随着模型规模增大需采用分布式计算，同时提供了关于跨 GPU 训练的可选视频。

W1 15 Optional video Efficient multi GPU compute strategies
本视频主要介绍了训练大型语言模型时面临的内存挑战及量化这一解决方案，包括不同精度数据类型对内存占用的影响及 B float 16 的优势，还提到了随着模型规模增大需采用分布式计算，同时提供了关于跨 GPU 训练的可选视频。

W1 16 Scaling Laws and Compute Optimal Models
本视频主要介绍了训练大型语言模型中模型规模、训练配置与性能的关系，包括计算资源的量化、不同模型的计算需求对比、计算预算与模型性能的关系，以及计算最优模型 Chinchilla 的研究成果和影响。

W1 17 Pre training for domain adaptation
视频主要介绍了在特定领域中可能需要从头预训练模型的情况，以实现更好的模型性能，还以 Bloomberg GPT 为例说明了预训练特定领域语言模型的方法，并回顾了本周所学内容。


W2 1 Introduction Week 2
大型语言模型的指令微调
W2 2 Instruction fine tuning
如何通过指令微调改进现有模型性能，包括指令微调的方法、重要指标、训练过程及步骤等内容。

W2 3 Fine tuning on a single task
大语言模型可在单一任务上微调以提升特定任务性能，但可能导致灾难性遗忘，讨论了避免灾难性遗忘的方法，包括确定其是否影响使用场景、同时在多个任务上微调、进行参数高效微调。

W2 4 Multi task instruction fine tuning
多任務指令微調（Multi task instruction fine tuning），包括其概念、FLAN 模型家族、以 FLAN T5 為例的提示模板及對特定任務的優化，並提到在實際應用中要考慮如何評估模型的完成質量。

W2 5 Model Evaluation
本视频主要介绍了大型语言模型的评估指标，包括 Rouge 和 BLEU，讲解了它们的计算方法和应用场景，强调不能单独使用这些指标进行最终评估，需结合其他评估基准。

rouge: unigram/bigram matching
BLEU: text translation compares to human generated translation

W2 6 Benchmarks
過既有資料集和基準對大語言模型進行更全面的評估，包括選擇合適的評估資料集

W2 7 Parameter efficient fine tuning PEFT
frozen weight
参数高效微调（PEFT）方法，对比全微调，PEFT 只更新少量参数子集或新增少量参数 / 层，减少训练内存需求，更易在单 GPU 上运行且不易出现灾难性遗忘问题，还介绍了几种 PEFT 方法及优缺点，最后引出特定软提示技术 prompt tuning 和下一个视频将介绍的 Laura 方法。

W2 8 PEFT techniques 1 LoRA
介紹了 PEFT 技術中的 LoRA（低秩適應），包括其工作原理、實例展示、性能對比、如何選擇矩陣秩等內容，強調其在減少可訓練參數、提高訓練效率方面的優勢。

W2 9 PEFT techniques 2 Soft prompts
视频介绍了参数高效微调技术（PEFT）中的软提示（soft prompts）方法，包括其与提示工程的区别、工作原理、性能表现及实际应用，还提到了另一种 PEFT 方法 Laura 及二者结合的 QLaura。

W2 10 Lab 2 walkthrough
本视频是关于 W2 10 Lab 2 的介绍，主要讲解了使用 flan T5 模型进行总结任务的全量微调及参数高效微调（Puft），包括模型加载、数据处理、训练过程以及通过 Rouge 指标评估不同微调策略的效果。
