import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
import seaborn as sns
import matplotlib.pyplot as plt

# ======== 1. 產生假資料集（類別數和分布不同） ========
np.random.seed(42)

n = 500

df = pd.DataFrame({
    'gender': np.random.choice(['Male', 'Female'], size=n, p=[0.4, 0.6]),
    'age_group': np.random.choice(['18-25', '26-35', '36-45', '46+'], size=n, p=[0.2, 0.4, 0.3, 0.1]),
    'product_type': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F'], size=n, p=[0.05, 0.1, 0.2, 0.3, 0.25, 0.1]),
    'region': np.random.choice(['North', 'South', 'East', 'West'], size=n, p=[0.2, 0.3, 0.25, 0.25]),
    'decision': np.random.choice(['Buy', 'Maybe', 'Not Buy'], size=n, p=[0.3, 0.4, 0.3])
})

# ======== 2. Cramér’s V + 類別數差異 + 分布熵調整 ========
def normalized_entropy(series):
    freq = series.value_counts(normalize=True)
    entropy = -(freq * np.log2(freq)).sum()
    max_entropy = np.log2(len(freq))
    return entropy / max_entropy if max_entropy > 0 else 0

def cramer_v_entropy_adjusted(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, c = confusion_matrix.shape
    k = min(r, c)

    v = np.sqrt(chi2 / (n * (k - 1))) if k > 1 else 0

    # 類別數差異懲罰
    class_diff = abs(r - c) / max(r, c) if max(r, c) > 0 else 0
    penalty_class = 1 - class_diff

    # 分布熵懲罰
    Hx = normalized_entropy(x)
    Hy = normalized_entropy(y)
    penalty_entropy = min(Hx, Hy)

    return v * penalty_class * penalty_entropy

# ======== 3. 建立欄位對欄位的關聯矩陣 ========
cols = df.columns
results = pd.DataFrame(index=cols, columns=cols, dtype=float)

for col1 in cols:
    for col2 in cols:
        if col1 == col2:
            results.loc[col1, col2] = 1.0
        else:
            results.loc[col1, col2] = cramer_v_entropy_adjusted(df[col1], df[col2])

# ======== 4. 顯示熱力圖 ========
plt.figure(figsize=(8, 6))
sns.heatmap(results.astype(float), annot=True, fmt=".2f", cmap="coolwarm", square=True)
plt.title("Adjusted Cramér’s V Heatmap (Class Count + Entropy Corrected)")
plt.tight_layout()
plt.show()


####################################
是否考量類別數量：

間接影響：Cramér’s V 通過 \text{min}(k-1, r-1)min(k−1,r−1) 來將卡方值正規化，因此類別數的大小會影響結果。例如，如果行列中類別數量差異很大，最大可能的 \chi^2χ 
2
  值也會受到限制，進而影響 V 值。
不直接考量差異：Cramér’s V 並不專門針對「兩類別數量是否相等」進行評估，它只考慮類別之間的關聯性強弱。
實際應用時的注意：

當兩個變數的類別數非常不平衡（例如，一個變數只有兩類，另一個有 10 類），這可能會影響解釋，但不會影響 Cramér’s V 的計算機制。
若要專門探討類別數差異的影響，則需進一步分析數據結構，而非僅依賴 Cramér’s V。

熱力圖顯示每對欄位間的「修正後 Cramér’s V」

已經考慮：

✅ 類別數差異（如 A 有 2 類，B 有 6 類 → 懲罰）

✅ 類別分布不平均（如 A 中 95% 都屬於一類 → 懲罰）
