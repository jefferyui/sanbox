import pandas as pd
from scipy.stats import chi2_contingency
import numpy as np
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import squareform
import matplotlib.pyplot as plt

# å®šç¾©å‡½æ•¸è¨ˆç®— CramÃ©r's Vï¼Œå¿½ç•¥ NaN å€¼
def cramers_V(contingency_table):
    chi2 = chi2_contingency(contingency_table)[0]
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))

def safe_cramers_V(series1, series2):
    # åˆªé™¤ NaN å€¼
    valid_idx = ~(series1.isna() | series2.isna())
    series1_valid = series1[valid_idx]
    series2_valid = series2[valid_idx]
    
    if len(series1_valid) < 2 or len(series2_valid) < 2:  # è‡³å°‘éœ€è¦ 2 å€‹ä¸åŒçš„å€¼
        return 0
    
    contingency_table = pd.crosstab(series1_valid, series2_valid)
    return cramers_V(contingency_table)

def safe_cramers_V(series1, series2):
    # åˆªé™¤ NaN å€¼
    valid_idx = ~(series1.isna() | series2.isna())
    series1_valid = series1[valid_idx]
    series2_valid = series2[valid_idx]
    
    if len(series1_valid) < 2 or len(series2_valid) < 2:  # è‡³å°‘éœ€è¦ 2 å€‹ä¸åŒçš„å€¼
        return 0
    
    contingency_table = pd.crosstab(series1_valid, series2_valid)
    
    # æª¢æŸ¥ contingency table çš„ç¶­åº¦
    if contingency_table.shape[0] == 1 or contingency_table.shape[1] == 1:
        return 0
    
    chi2 = chi2_contingency(contingency_table)[0]
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))

# å‡è¨­æœ‰ä»¥ä¸‹çš„é¡åˆ¥å‹æ•¸æ“šï¼Œå«æœ‰ NaN å€¼
# data = pd.DataFrame({
#     'A': np.random.choice(['a1', 'a2', 'a3', np.nan], 100),
#     'B': np.random.choice(['b1', 'b2', np.nan], 100),
#     'C': np.random.choice(['c1', 'c2', 'c3'], 100),
#     'D': np.random.choice(['d1', 'd2', np.nan], 100),
#     'E': np.random.choice(['e1', 'e2', 'e3'], 100),
#     'F': np.random.choice(['f1', 'f2'], 100)
# })

# data = pd.DataFrame({
#     'Gender':    ['M', 'F', 'F', 'M', 'F', 'M', np.nan, 'F'],
#     'Education': ['HS', np.nan, 'HS', 'MS', 'PhD', 'MS', 'HS', 'PhD'],
#     'Choice':    ['A', 'B', 'A', 'B', np.nan, 'A', 'B', 'B'],
#     'Region':    ['North', 'South', 'South', 'North', 'East', 'East', 'West', np.nan]
# })

data = pd.DataFrame({
    'Gender':    ['M', 'F', 'F', 'M', 'F', 'M', 'M', 'F'],
    'Education': ['HS', 'PhD', 'HS', 'MS', 'PhD', 'MS', 'HS', 'PhD'],
    'Choice':    ['A', 'B', 'A', 'B', 'A', 'A', 'B', 'B'],
    'Region':    ['North', 'South', 'South', 'North', 'East', 'East', 'West', 'South']
})

# è¨ˆç®— CramÃ©r's V çŸ©é™£ï¼Œå¿½ç•¥ NaN å€¼
columns = data.columns
cramers_v_matrix = np.zeros((len(columns), len(columns)))

for i in range(len(columns)):
    for j in range(i, len(columns)):
        if i == j:
            cramers_v_matrix[i, j] = 1
        else:
            cramers_v = safe_cramers_V(data[columns[i]], data[columns[j]])
            cramers_v_matrix[i, j] = cramers_v
            cramers_v_matrix[j, i] = cramers_v

# è½‰æ›ç‚ºè·é›¢çŸ©é™£
distance_matrix = 1 - cramers_v_matrix

# å°‡ squareform çš„ distance matrix è½‰æ›ç‚º condensed å½¢å¼
condensed_distance_matrix = squareform(distance_matrix, checks=False)

# é€²è¡Œå±¤æ¬¡èšé¡
Z = linkage(condensed_distance_matrix, method='ward')

# ç¹ªè£½æ¨¹ç‹€åœ–
plt.figure(figsize=(10, 7))
dendrogram(Z, labels=columns)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

# ä½¿ç”¨è·é›¢é–¾å€¼æ±ºå®šèšé¡æ•¸
distance_threshold = 0.5
cluster_labels = fcluster(Z, t=distance_threshold, criterion='distance')

# å°‡æ¬„ä½åˆ†çµ„ä¸¦é¸å–ä»£è¡¨
cluster_dict = {}
for i, label in enumerate(cluster_labels):
    if label not in cluster_dict:
        cluster_dict[label] = []
    cluster_dict[label].append(columns[i])

# é¸å–æ¯çµ„çš„ç¬¬ä¸€å€‹æ¬„ä½ä½œç‚ºä»£è¡¨
representatives = {label: cluster[0] for label, cluster in cluster_dict.items()}

# å»ºç«‹çµæœ DataFrame
result_df = pd.DataFrame({
    'ä»£è¡¨': [representatives[label] for label in cluster_dict.keys()],
    'ç¾¤çµ„': [', '.join(cluster) for cluster in cluster_dict.values()]
})

print(result_df)


#####################################################################################
é€™æ™‚é¡åˆ¥æ•¸å¢åŠ ï¼Œåˆ†å¸ƒä¸å‡ï¼ŒCramÃ©râ€™s V é›–ç„¶å¯ä»¥ç®—ï¼Œä½†çµæœè¦å°å¿ƒè§£é‡‹ã€‚

âœ… çµè«–
CramÃ©râ€™s V ä¸æœƒä¸»å‹•åˆ¤æ–·é¡åˆ¥æ•¸å·®ç•°

ä½†å®ƒçš„è¨ˆç®—æœƒè€ƒæ…®è¼ƒå°çš„é¡åˆ¥æ•¸ä½œç‚ºæ¨™æº–åŒ–å› å­

é¡åˆ¥æ•¸å·®ç•°å¤§ â†’ å¯èƒ½å½±éŸ¿æŒ‡æ¨™ç©©å®šæ€§èˆ‡å¯è§£é‡‹æ€§ï¼Œä½†ä¸å½±éŸ¿å…¬å¼æœ¬èº«é‹ä½œ
CramÃ©râ€™s V ä¸æœƒä¸»å‹•åˆ¤æ–·é¡åˆ¥æ•¸å·®ç•°ï¼Œä½†æˆ‘è¦åŠ å…¥å…©é¡åˆ¥æ•¸çš„åˆ¤æ–·è¦å¦‚ä½•

def category_difference_score(x, y):
    n_x = x.nunique()
    n_y = y.nunique()
    return abs(n_x - n_y) / max(n_x, n_y)
ç¯„åœï¼š0 ~ 1

0ï¼šå…©å€‹è®Šæ•¸çš„é¡åˆ¥æ•¸ç›¸åŒ

è¶Šæ¥è¿‘ 1ï¼Œé¡åˆ¥æ•¸å·®ç•°è¶Šå¤§

if cramer_v > 0.3 and category_difference_score(x, y) < 0.2:
    print("é—œè¯å¼·ä¸”é¡åˆ¥æ•¸ç›¸è¿‘")
elif cramer_v > 0.3 and category_difference_score(x, y) > 0.5:
    print("é—œè¯å¼·ä½†é¡åˆ¥æ•¸å·®ç•°å¤§ï¼Œéœ€å°å¿ƒè§£é‡‹")


å¦‚æœä½ æƒ³æª¢æŸ¥ã€Œé¡åˆ¥æ•¸å·®ç•°æ˜¯å¦é¡¯è‘—å½±éŸ¿é—œè¯ã€ï¼Œå¯ä»¥è€ƒæ…®æ­é…ï¼š

Theilâ€™s Uï¼ˆéå°ç¨±æŒ‡æ¨™ï¼‰

Mutual Information

ç²¾ç¢ºçš„å¡æ–¹æª¢å®šï¼ˆExact Testï¼‰ æˆ– æ¨¡æ“¬æ–¹æ³•

âœ… æ–¹æ³• 2ï¼šæ”¹ç”¨ä¸å°ç¨±æŒ‡æ¨™ Theilâ€™s Uï¼ˆU(x|y) â‰  U(y|x)ï¼‰
Theilâ€™s Uï¼ˆUncertainty Coefficientï¼‰æ˜¯ä¸€å€‹æ ¹æ“šè³‡è¨Šç†µçš„æŒ‡æ¨™ï¼Œå¯ä»¥åæ˜ æŸå€‹è®Šæ•¸å°å¦ä¸€å€‹è®Šæ•¸çš„ã€Œé æ¸¬åŠ›ã€ï¼Œä¸¦è‡ªç„¶è€ƒæ…®é¡åˆ¥æ•¸ã€‚

Theilâ€™s U çš„ç‰¹æ€§ï¼š
è€ƒæ…®æ–¹å‘æ€§ï¼ˆU(x|y) â‰  U(y|x)ï¼‰

å¦‚æœ y æœ‰å¾ˆå¤šé¡åˆ¥ä½† x æ²’æœ‰ï¼Œé‚£ U(x|y) æœƒå° â†’ é¡¯ç¤º x é›£ä»¥é æ¸¬ y

å¯ä»¥å½Œè£œ CramÃ©râ€™s V å°ä¸å°ç¨±èˆ‡é¡åˆ¥ä¸å¹³è¡¡çš„å¿½ç•¥

ğŸ›  Python å¥—ä»¶ dython å¯ä»¥è¨ˆç®—ï¼š

python
è¤‡è£½
ç·¨è¼¯
from dython.nominal import theils_u

u_xy = theils_u(x, y)
u_yx = theils_u(y, x)
ä½ å¯ä»¥é€™æ¨£çµåˆåˆ†æï¼š

python
è¤‡è£½
ç·¨è¼¯
if cramer_v > 0.3 and abs(u_xy - u_yx) > 0.2:
    print("é—œè¯æ€§å­˜åœ¨ï¼Œä½†æ–¹å‘æ€§èˆ‡é¡åˆ¥ä¸å°ç¨±éœ€è€ƒæ…®")


#########################################################
å¦‚ä½•ç›´æ¥æ”¹è‰¯cramer_v ç®—å¼åŠ å…¥å…©å€‹è®Šæ•¸çš„é¡åˆ¥æ•¸å·®ç•°å¤§å°


ChatGPT èªªï¼š
å¾ˆæ£’çš„å•é¡Œï¼ä½ æƒ³è¦ç›´æ¥æ”¹è‰¯ CramÃ©râ€™s V çš„å…¬å¼ä¾†ç´å…¥å…©è®Šæ•¸é¡åˆ¥æ•¸çš„å·®ç•°å½±éŸ¿ï¼Œé€™æ˜¯ä¸€ç¨®å…·æœ‰å¯¦å‹™è€ƒé‡çš„æŒ‡æ¨™èª¿æ•´ã€‚


ğŸ¯ ç›®æ¨™
è®“æ”¹è‰¯å¾Œçš„ CramÃ©râ€™s V ä¸åªè€ƒæ…®é—œè¯å¼·åº¦ï¼Œé‚„æ‡²ç½°æˆ–ä¿®æ­£é¡åˆ¥æ•¸ä¸å°ç¨±çš„å½±éŸ¿ã€‚

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

def cramer_v_adjusted(x, y, alpha=1.0, method='linear'):
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, c = confusion_matrix.shape
    k = min(r, c)

    v = np.sqrt(chi2 / (n * (k - 1)))

    # é¡åˆ¥å·®ç•°ä¿®æ­£å› å­
    diff = abs(r - c) / max(r, c)
    if method == 'linear':
        penalty = 1 - diff
    elif method == 'exp':
        penalty = np.exp(-alpha * diff)
    else:
        raise ValueError("method must be 'linear' or 'exp'")

    return v * penalty

cramer_v_adjusted(df['æ€§åˆ¥'], df['è³¼è²·æ„é¡˜'], alpha=1.0, method='exp')
ğŸ§  è§£é‡‹æ€§è£œå……
é€™äº›èª¿æ•´ ä¸æ˜¯çµ±è¨ˆå­¸ä¸Šåš´æ ¼çš„ä¿®æ­£ï¼Œä½†åœ¨å¯¦å‹™æ‡‰ç”¨ä¸­ç‰¹åˆ¥æœ‰ç”¨

ç‰¹åˆ¥æ˜¯åœ¨é¡åˆ¥æ•¸ä¸å¹³è¡¡ã€æˆ–ä¸å°ç¨±è¡¨æ ¼ä¸­ä½¿ç”¨æ™‚ï¼Œæœƒè®“çµæœæ›´ç©©å®šèˆ‡å¯æ¯”è¼ƒ

è‹¥ä½ æœ‰å¤§é‡é¡åˆ¥è®Šæ•¸é…å°åšé—œè¯åˆ†æï¼Œé€™ç¨®ã€Œæ‡²ç½°é¡åˆ¥å·®ç•°ã€çš„æ–¹æ³•èƒ½å¹«åŠ©ä½ å¿«é€Ÿæ’é™¤ä¸ç©©å®šé—œè¯

é‡å°ã€Œé »ç‡åˆ†å¸ƒã€å†ä¿®æ­£ï¼ˆå¦‚é¡åˆ¥åˆ†å¸ƒç†µï¼‰


ChatGPT èªªï¼š
å¾ˆå¥½ï¼ä½ æƒ³å°‡ CramÃ©râ€™s V çš„æ”¹è‰¯ç‰ˆå†ç´å…¥ã€Œé¡åˆ¥åˆ†å¸ƒï¼ˆé »ç‡åˆ†å¸ƒï¼‰ä¸å‡ã€çš„å½±éŸ¿ï¼Œé€™æ˜¯é€²ä¸€æ­¥è§£æ±ºï¼š

é¡åˆ¥é›–ç„¶æ•¸ç›®ç›¸åŒï¼Œä½†åˆ†å¸ƒæ¥µä¸å‡è¡¡ï¼ˆå¦‚é•·å°¾é¡åˆ¥ï¼‰æ™‚ CramÃ©râ€™s V ä»å¯èƒ½åé«˜çš„å•é¡Œã€‚

å¾ˆå¥½ï¼ä½ æƒ³å°‡ CramÃ©râ€™s V çš„æ”¹è‰¯ç‰ˆå†ç´å…¥ã€Œé¡åˆ¥åˆ†å¸ƒï¼ˆé »ç‡åˆ†å¸ƒï¼‰ä¸å‡ã€çš„å½±éŸ¿ï¼Œé€™æ˜¯é€²ä¸€æ­¥è§£æ±ºï¼š

é¡åˆ¥é›–ç„¶æ•¸ç›®ç›¸åŒï¼Œä½†åˆ†å¸ƒæ¥µä¸å‡è¡¡ï¼ˆå¦‚é•·å°¾é¡åˆ¥ï¼‰æ™‚ CramÃ©râ€™s V ä»å¯èƒ½åé«˜çš„å•é¡Œã€‚

ğŸ¯ ç›®æ¨™
é™¤äº†åŸæœ¬ä¿®æ­£ã€Œé¡åˆ¥æ•¸å·®ç•°ã€å¤–ï¼Œå†åŠ å…¥ï¼š

é¡åˆ¥åˆ†å¸ƒè¶Šä¸å¹³å‡ âœ é—œè¯å¼·åº¦è¶Šéœ€æ‡²ç½°

ä½¿ç”¨**è³‡è¨Šç†µï¼ˆEntropyï¼‰**æˆ–å…¶æ¨™æº–åŒ–å½¢å¼ä¾†è¡¡é‡é¡åˆ¥åˆ†å¸ƒçš„å‡å‹»ç¨‹åº¦

 æ”¹è‰¯ CramÃ©râ€™s V with åˆ†å¸ƒç†µä¿®æ­£ï¼ˆå®Œæ•´å…¬å¼ï¼‰

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

def normalized_entropy(series):
    freq = series.value_counts(normalize=True)
    entropy = -(freq * np.log2(freq)).sum()
    max_entropy = np.log2(len(freq))
    return entropy / max_entropy if max_entropy > 0 else 0

def cramer_v_entropy_adjusted(x, y):
    # å»ºç«‹åˆ—è¯è¡¨
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, c = confusion_matrix.shape
    k = min(r, c)

    # åŸå§‹ CramÃ©râ€™s V
    v = np.sqrt(chi2 / (n * (k - 1)))

    # ä¿®æ­£å› å­ 1ï¼šé¡åˆ¥æ•¸å·®ç•°
    class_diff = abs(r - c) / max(r, c)
    penalty_class_count = 1 - class_diff

    # ä¿®æ­£å› å­ 2ï¼šé »ç‡åˆ†å¸ƒç†µ
    Hx = normalized_entropy(x)
    Hy = normalized_entropy(y)
    penalty_entropy = min(Hx, Hy)

    # æœ€çµ‚æ”¹è‰¯ç‰ˆ V
    return v * penalty_class_count * penalty_entropy
cramer_v_entropy_adjusted(df["ç”¢å“é¡å‹"], df["è³¼è²·æ„é¡˜"])
| ä¿®æ­£é …ç›®    | è§£æ±ºä»€éº¼å•é¡Œ          |
| ------- | --------------- |
| é¡åˆ¥æ•¸å·®ç•°èª¿æ•´ | é¡åˆ¥æ•¸ç›®æ¥µä¸å¹³è¡¡æ™‚é¿å…åé«˜   |
| é¡åˆ¥åˆ†å¸ƒç†µæ‡²ç½° | é•·å°¾ã€ä¸å‡åˆ†å¸ƒæ™‚é¿å…èª¤åˆ¤å¼·é—œè¯ |

