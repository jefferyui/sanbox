import pandas as pd
from scipy.stats import chi2_contingency
import numpy as np
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import squareform
import matplotlib.pyplot as plt

# 定義函數計算 Cramér's V，忽略 NaN 值
def cramers_V(contingency_table):
    chi2 = chi2_contingency(contingency_table)[0]
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))

def safe_cramers_V(series1, series2):
    # 刪除 NaN 值
    valid_idx = ~(series1.isna() | series2.isna())
    series1_valid = series1[valid_idx]
    series2_valid = series2[valid_idx]
    
    if len(series1_valid) < 2 or len(series2_valid) < 2:  # 至少需要 2 個不同的值
        return 0
    
    contingency_table = pd.crosstab(series1_valid, series2_valid)
    return cramers_V(contingency_table)

def safe_cramers_V(series1, series2):
    # 刪除 NaN 值
    valid_idx = ~(series1.isna() | series2.isna())
    series1_valid = series1[valid_idx]
    series2_valid = series2[valid_idx]
    
    if len(series1_valid) < 2 or len(series2_valid) < 2:  # 至少需要 2 個不同的值
        return 0
    
    contingency_table = pd.crosstab(series1_valid, series2_valid)
    
    # 檢查 contingency table 的維度
    if contingency_table.shape[0] == 1 or contingency_table.shape[1] == 1:
        return 0
    
    chi2 = chi2_contingency(contingency_table)[0]
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))

# 假設有以下的類別型數據，含有 NaN 值
# data = pd.DataFrame({
#     'A': np.random.choice(['a1', 'a2', 'a3', np.nan], 100),
#     'B': np.random.choice(['b1', 'b2', np.nan], 100),
#     'C': np.random.choice(['c1', 'c2', 'c3'], 100),
#     'D': np.random.choice(['d1', 'd2', np.nan], 100),
#     'E': np.random.choice(['e1', 'e2', 'e3'], 100),
#     'F': np.random.choice(['f1', 'f2'], 100)
# })

# data = pd.DataFrame({
#     'Gender':    ['M', 'F', 'F', 'M', 'F', 'M', np.nan, 'F'],
#     'Education': ['HS', np.nan, 'HS', 'MS', 'PhD', 'MS', 'HS', 'PhD'],
#     'Choice':    ['A', 'B', 'A', 'B', np.nan, 'A', 'B', 'B'],
#     'Region':    ['North', 'South', 'South', 'North', 'East', 'East', 'West', np.nan]
# })

data = pd.DataFrame({
    'Gender':    ['M', 'F', 'F', 'M', 'F', 'M', 'M', 'F'],
    'Education': ['HS', 'PhD', 'HS', 'MS', 'PhD', 'MS', 'HS', 'PhD'],
    'Choice':    ['A', 'B', 'A', 'B', 'A', 'A', 'B', 'B'],
    'Region':    ['North', 'South', 'South', 'North', 'East', 'East', 'West', 'South']
})

# 計算 Cramér's V 矩陣，忽略 NaN 值
columns = data.columns
cramers_v_matrix = np.zeros((len(columns), len(columns)))

for i in range(len(columns)):
    for j in range(i, len(columns)):
        if i == j:
            cramers_v_matrix[i, j] = 1
        else:
            cramers_v = safe_cramers_V(data[columns[i]], data[columns[j]])
            cramers_v_matrix[i, j] = cramers_v
            cramers_v_matrix[j, i] = cramers_v

# 轉換為距離矩陣
distance_matrix = 1 - cramers_v_matrix

# 將 squareform 的 distance matrix 轉換為 condensed 形式
condensed_distance_matrix = squareform(distance_matrix, checks=False)

# 進行層次聚類
Z = linkage(condensed_distance_matrix, method='ward')

# 繪製樹狀圖
plt.figure(figsize=(10, 7))
dendrogram(Z, labels=columns)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

# 使用距離閾值決定聚類數
distance_threshold = 0.5
cluster_labels = fcluster(Z, t=distance_threshold, criterion='distance')

# 將欄位分組並選取代表
cluster_dict = {}
for i, label in enumerate(cluster_labels):
    if label not in cluster_dict:
        cluster_dict[label] = []
    cluster_dict[label].append(columns[i])

# 選取每組的第一個欄位作為代表
representatives = {label: cluster[0] for label, cluster in cluster_dict.items()}

# 建立結果 DataFrame
result_df = pd.DataFrame({
    '代表': [representatives[label] for label in cluster_dict.keys()],
    '群組': [', '.join(cluster) for cluster in cluster_dict.values()]
})

print(result_df)


#####################################################################################
這時類別數增加，分布不均，Cramér’s V 雖然可以算，但結果要小心解釋。

✅ 結論
Cramér’s V 不會主動判斷類別數差異

但它的計算會考慮較小的類別數作為標準化因子

類別數差異大 → 可能影響指標穩定性與可解釋性，但不影響公式本身運作
Cramér’s V 不會主動判斷類別數差異，但我要加入兩類別數的判斷要如何

def category_difference_score(x, y):
    n_x = x.nunique()
    n_y = y.nunique()
    return abs(n_x - n_y) / max(n_x, n_y)
範圍：0 ~ 1

0：兩個變數的類別數相同

越接近 1，類別數差異越大

if cramer_v > 0.3 and category_difference_score(x, y) < 0.2:
    print("關聯強且類別數相近")
elif cramer_v > 0.3 and category_difference_score(x, y) > 0.5:
    print("關聯強但類別數差異大，需小心解釋")


如果你想檢查「類別數差異是否顯著影響關聯」，可以考慮搭配：

Theil’s U（非對稱指標）

Mutual Information

精確的卡方檢定（Exact Test） 或 模擬方法

✅ 方法 2：改用不對稱指標 Theil’s U（U(x|y) ≠ U(y|x)）
Theil’s U（Uncertainty Coefficient）是一個根據資訊熵的指標，可以反映某個變數對另一個變數的「預測力」，並自然考慮類別數。

Theil’s U 的特性：
考慮方向性（U(x|y) ≠ U(y|x)）

如果 y 有很多類別但 x 沒有，那 U(x|y) 會小 → 顯示 x 難以預測 y

可以彌補 Cramér’s V 對不對稱與類別不平衡的忽略

🛠 Python 套件 dython 可以計算：

python
複製
編輯
from dython.nominal import theils_u

u_xy = theils_u(x, y)
u_yx = theils_u(y, x)
你可以這樣結合分析：

python
複製
編輯
if cramer_v > 0.3 and abs(u_xy - u_yx) > 0.2:
    print("關聯性存在，但方向性與類別不對稱需考慮")


#########################################################
如何直接改良cramer_v 算式加入兩個變數的類別數差異大小


ChatGPT 說：
很棒的問題！你想要直接改良 Cramér’s V 的公式來納入兩變數類別數的差異影響，這是一種具有實務考量的指標調整。


🎯 目標
讓改良後的 Cramér’s V 不只考慮關聯強度，還懲罰或修正類別數不對稱的影響。

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

def cramer_v_adjusted(x, y, alpha=1.0, method='linear'):
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, c = confusion_matrix.shape
    k = min(r, c)

    v = np.sqrt(chi2 / (n * (k - 1)))

    # 類別差異修正因子
    diff = abs(r - c) / max(r, c)
    if method == 'linear':
        penalty = 1 - diff
    elif method == 'exp':
        penalty = np.exp(-alpha * diff)
    else:
        raise ValueError("method must be 'linear' or 'exp'")

    return v * penalty

cramer_v_adjusted(df['性別'], df['購買意願'], alpha=1.0, method='exp')
🧠 解釋性補充
這些調整 不是統計學上嚴格的修正，但在實務應用中特別有用

特別是在類別數不平衡、或不對稱表格中使用時，會讓結果更穩定與可比較

若你有大量類別變數配對做關聯分析，這種「懲罰類別差異」的方法能幫助你快速排除不穩定關聯

針對「頻率分布」再修正（如類別分布熵）


ChatGPT 說：
很好！你想將 Cramér’s V 的改良版再納入「類別分布（頻率分布）不均」的影響，這是進一步解決：

類別雖然數目相同，但分布極不均衡（如長尾類別）時 Cramér’s V 仍可能偏高的問題。

很好！你想將 Cramér’s V 的改良版再納入「類別分布（頻率分布）不均」的影響，這是進一步解決：

類別雖然數目相同，但分布極不均衡（如長尾類別）時 Cramér’s V 仍可能偏高的問題。

🎯 目標
除了原本修正「類別數差異」外，再加入：

類別分布越不平均 ➜ 關聯強度越需懲罰

使用**資訊熵（Entropy）**或其標準化形式來衡量類別分布的均勻程度

 改良 Cramér’s V with 分布熵修正（完整公式）

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency

def normalized_entropy(series):
    freq = series.value_counts(normalize=True)
    entropy = -(freq * np.log2(freq)).sum()
    max_entropy = np.log2(len(freq))
    return entropy / max_entropy if max_entropy > 0 else 0

def cramer_v_entropy_adjusted(x, y):
    # 建立列聯表
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, c = confusion_matrix.shape
    k = min(r, c)

    # 原始 Cramér’s V
    v = np.sqrt(chi2 / (n * (k - 1)))

    # 修正因子 1：類別數差異
    class_diff = abs(r - c) / max(r, c)
    penalty_class_count = 1 - class_diff

    # 修正因子 2：頻率分布熵
    Hx = normalized_entropy(x)
    Hy = normalized_entropy(y)
    penalty_entropy = min(Hx, Hy)

    # 最終改良版 V
    return v * penalty_class_count * penalty_entropy
cramer_v_entropy_adjusted(df["產品類型"], df["購買意願"])
| 修正項目    | 解決什麼問題          |
| ------- | --------------- |
| 類別數差異調整 | 類別數目極不平衡時避免偏高   |
| 類別分布熵懲罰 | 長尾、不均分布時避免誤判強關聯 |

