import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor, KernelDensity, NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import MinMaxScaler

# ----------------------------
# 1. 生成模擬資料 (2D -> 1D)
# ----------------------------
rng = np.random.RandomState(42)
X_inliers = 0.3 * rng.randn(100, 2)
X_outliers = rng.uniform(low=-4, high=4, size=(10, 2))
X = np.vstack([X_inliers, X_outliers])

# 壓成 1D (投影)
X_1d = X[:, 0] + X[:, 1]   # 簡單投影方式
X_1d = X_1d.reshape(-1, 1)

# ----------------------------
# 2. 各種方法偵測 outlier
# ----------------------------

# (a) LOF
lof = LocalOutlierFactor(n_neighbors=10, contamination='auto')
y_pred_lof = lof.fit_predict(X_1d)
lof_outliers = y_pred_lof == -1

# (b) DBSCAN
db = DBSCAN(eps=2, min_samples=10).fit(X_1d)
db_outliers = db.labels_ == -1

# (c) KDE
kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X_1d)
log_density = kde.score_samples(X_1d)
threshold = np.percentile(log_density, 10)  # 取最低 10% 當作 outlier
kde_outliers = log_density < threshold

# (d) kNN Distance
nbrs = NearestNeighbors(n_neighbors=20).fit(X_1d)
distances, _ = nbrs.kneighbors(X_1d)
knn_score = distances[:, -1]  # 距離第 k 個最近鄰
th_knn = np.percentile(knn_score, 90)  # 最大 10% 當作 outlier
knn_outliers = knn_score > th_knn

# ----------------------------
# 3. 畫圖比較
# ----------------------------
methods = {
    "LOF": lof_outliers,
    "DBSCAN": db_outliers,
    "KDE": kde_outliers,
    "kNN Distance": knn_outliers
}

plt.figure(figsize=(12, 8))
for i, (name, mask) in enumerate(methods.items(), 1):
    plt.subplot(2, 2, i)
    plt.scatter(np.arange(len(X_1d)), X_1d, c="blue", s=20, label="normal")
    plt.scatter(np.where(mask)[0], X_1d[mask], c="red", s=50, label="outlier")
    plt.title(name)
    plt.legend()

plt.tight_layout()
plt.show()

##############################################################
import numpy as np
import matplotlib.pyplot as plt
import time
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN, MeanShift
import hdbscan
from memory_profiler import memory_usage

# 產生大數據 (10萬筆資料, 5個群)
X, _ = make_blobs(n_samples=100000, centers=5, cluster_std=0.60, random_state=42)

def run_with_memory(func, *args, **kwargs):
    """測量函數的記憶體與執行時間"""
    start_time = time.time()
    mem_usage = memory_usage((func, args, kwargs), max_iterations=1, interval=0.1)
    end_time = time.time()
    return func(*args, **kwargs), max(mem_usage), end_time - start_time

# --- DBSCAN ---
def run_dbscan(X):
    model = DBSCAN(eps=0.5, min_samples=5, algorithm='ball_tree')
    return model.fit_predict(X)

labels_db, mem_db, time_db = run_with_memory(run_dbscan, X)
print(f"DBSCAN: clusters={len(set(labels_db))}, mem={mem_db:.2f} MiB, time={time_db:.2f} sec")

# --- HDBSCAN ---
def run_hdbscan(X):
    model = hdbscan.HDBSCAN(min_cluster_size=20)
    return model.fit_predict(X)

labels_hdb, mem_hdb, time_hdb = run_with_memory(run_hdbscan, X)
print(f"HDBSCAN: clusters={len(set(labels_hdb))}, mem={mem_hdb:.2f} MiB, time={time_hdb:.2f} sec")

# --- MeanShift (DENCLUE approximation) ---
def run_meanshift(X):
    model = MeanShift(bandwidth=1.0, bin_seeding=True)
    return model.fit_predict(X)

labels_ms, mem_ms, time_ms = run_with_memory(run_meanshift, X)
print(f"MeanShift (DENCLUE): clusters={len(set(labels_ms))}, mem={mem_ms:.2f} MiB, time={time_ms:.2f} sec")

def lof(X):
    model = LocalOutlierFactor(n_neighbors=10, contamination='auto')
    return lof.fit_predict(X_1d)

labels_ms, mem_ms, time_ms = run_with_memory(run_meanshift, X)
print(f"lof: clusters={len(set(labels_ms))}, mem={mem_ms:.2f} MiB, time={time_ms:.2f} sec")

# --- 畫圖 (取前5000筆避免太擠) ---
plt.figure(figsize=(12, 4))
for i, (labels, title) in enumerate([
    (labels_db, "DBSCAN"),
    (labels_hdb, "HDBSCAN"),
    (labels_ms, "MeanShift (DENCLUE approx.)")
]):
    plt.subplot(1, 3, i+1)
    plt.scatter(X[:5000, 0], X[:5000, 1], c=labels[:5000], cmap="rainbow", s=5)
    plt.title(title)
plt.show()

