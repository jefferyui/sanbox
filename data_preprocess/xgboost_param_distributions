import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBRegressor
import matplotlib.pyplot as plt

# 生成數據
np.random.seed(42)
X = np.random.rand(1000, 10)
y = X[:, 0] * 10 + np.random.normal(0, 1, 1000)

# 分割數據集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定義 XGBoost 模型和隨機搜索範圍
param_distributions = {
    'max_depth': [3, 4, 5, 6],         # 樹的深度
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # 學習速率
    'n_estimators': [100, 200, 300, 500],     # 樹的數量
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的樣本比例
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的特徵比例
    'reg_alpha': [0, 0.01, 0.1, 1],   # L1 正則化
    'reg_lambda': [1, 5, 10, 20]      # L2 正則化
}

# 建立 XGBRegressor 實例
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)

# 使用 RandomizedSearchCV 進行隨機搜索
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_distributions,
    scoring='neg_mean_squared_error',  # 使用負均方誤差作為評分標準
    n_iter=50,                         # 隨機搜索的迭代次數
    cv=3,                              # 3 折交叉驗證
    random_state=42,
    verbose=1,                         # 顯示訓練過程
    n_jobs=-1                          # 使用所有 CPU 加速計算
)

# 執行隨機搜索
random_search.fit(X_train, y_train)

# 輸出最佳超參數
print("Best parameters found by Random Search:")
print(random_search.best_params_)

# 使用最佳參數訓練最終模型
best_model = random_search.best_estimator_

# 預測
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

# 計算 R² 和 RMSE
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f"Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}")
print(f"Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}")

# 可視化訓練集與測試集的 RMSE 比較（如有必要）
plt.bar(['Train RMSE', 'Test RMSE'], [train_rmse, test_rmse], color=['blue', 'orange'])
plt.ylabel('RMSE')
plt.title('Train vs Test RMSE')
plt.show()


調參建議
避免過擬合：

檢查 Train R² 和 Test R²，如果兩者差距過大，可能有過擬合問題。
適當減小 max_depth 或增加正則化參數 reg_alpha, reg_lambda。
增加泛化能力：

調整 subsample 和 colsample_bytree，減少使用過多樣本與特徵。
使用多次隨機搜索：

如果結果仍不理想，可適當增加 n_iter 或引入更多的參數範圍進行搜索。

# # 3. 設定參數 (避免 overfitting)
# params = {
#     "objective": "reg:squarederror",   # 回歸
#     "eval_metric": "rmse",            # 評估指標
#     "max_depth": 4,                   # 降低樹的深度 (避免太複雜)
#     "min_child_weight": 5,            # 避免小樣本分裂
#     "subsample": 0.8,                 # 隨機取樣
#     "colsample_bytree": 0.8,          # 特徵取樣
#     "learning_rate": 0.05,            # 降低學習率
#     "reg_alpha": 0.1,                 # L1 正則化
#     "reg_lambda": 1.0                 # L2 正則化
# }

| 問題情境               | 可能原因    | 調整方法       | Python 參數設定                         |
| ------------------ | ------- | ---------- | ----------------------------------- |
| 訓練誤差很低，但 test 誤差很高 | 樹太複雜    | 減少深度       | `max_depth` ↓ (例如 10 → 4\~6)        |
| 模型過度依賴少量樣本分裂       | 樹分裂過細   | 增加子節點樣本需求  | `min_child_weight` ↑ (例如 1 → 5\~10) |
| 樣本學太「死」            | 資料沒有隨機性 | 增加隨機取樣     | `subsample` < 1.0 (例如 0.8)          |
| 特徵太多，每次都用全部        | 特徵取樣不足  | 降低特徵取樣比例   | `colsample_bytree` < 1.0 (例如 0.8)   |
| 權重過大，模型偏向複雜        | 正則化不足   | 增加正則化      | `reg_alpha` (L1)，`reg_lambda` (L2)  |
| 學習太快，提早 overfit    | 學習率太高   | 降低學習率並增加樹數 | `learning_rate` ↓，`n_estimators` ↑  |
| 訓練輪數過多             | 過度訓練    | 加入早停       | `early_stopping_rounds=30`          |


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from xgboost import XGBRegressor

# 生成數據
np.random.seed(42)
X = np.random.rand(1000, 10)
y = X[:, 0] * 10 + np.random.normal(0, 1, 1000)

# 分割數據集
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 再次分割，生成驗證集

# 超參數搜索範圍
param_distributions = {
    'max_depth': [3, 4, 5, 6],         # 樹的深度
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # 學習率
    'n_estimators': [100, 200, 300, 500],     # 樹的數量
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的樣本比例
    "min_child_weight":  [1, 3, 5],            # 避免小樣本分裂
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的特徵比例
    'reg_alpha': [0, 0.01, 0.1, 1],   # L1 正則化
    'reg_lambda': [1, 5, 10, 20]      # L2 正則化
}

# 初始化 XGBoost 模型
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)

# 使用 RandomizedSearchCV 進行隨機搜索
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_distributions,
    scoring='neg_mean_squared_error',  # 使用負均方誤差作為評分標準
    n_iter=50,                         # 隨機搜索的嘗試次數
    cv=3,                              # 3 折交叉驗證
    random_state=42,
    verbose=1,                         # 顯示訓練過程
    n_jobs=-1                          # 使用所有 CPU 加速計算
)

# 執行隨機搜索
random_search.fit(X_train, y_train)

# 輸出最佳超參數
print("Best parameters found by Random Search:")
print(random_search.best_params_)

# 使用最佳參數再次訓練模型，加入 Early Stopping
best_params = random_search.best_params_

# 訓練模型並加入 Early Stopping
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# 更新參數，移除 n_estimators，改用 early stopping
best_params.update({
    "objective": "reg:squarederror",
    "eval_metric": "rmse",  # 指標為 RMSE
})
evals = [(dtrain, 'train'), (dvalid, 'validate')]

xgb_model_final = xgb.train(
    params=best_params,
    dtrain=dtrain,
    num_boost_round=500,               # 設置最大迭代次數
    evals=evals,
    early_stopping_rounds=50,          # 50 輪內若無提升則停止訓練
    verbose_eval=10                    # 每 10 輪顯示一次訓練結果
)

# 預測結果
y_train_pred = xgb_model_final.predict(dtrain)
y_valid_pred = xgb_model_final.predict(dvalid)
y_test_pred = xgb_model_final.predict(dtest)

# 計算 R² 和 RMSE
train_r2 = r2_score(y_train, y_train_pred)
valid_r2 = r2_score(y_valid, y_valid_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

# 打印結果
print("\nModel performance:")
print(f"Train R²: {train_r2:.4f} | Valid R²: {valid_r2:.4f} | Test R²: {test_r2:.4f}")
print(f"Train RMSE: {train_rmse:.4f} | Valid RMSE: {valid_rmse:.4f} | Test RMSE: {test_rmse:.4f}")

# 可視化 RMSE
plt.bar(['Train RMSE', 'Valid RMSE', 'Test RMSE'], [train_rmse, valid_rmse, test_rmse], color=['blue', 'green', 'orange'])
plt.ylabel('RMSE')
plt.title('Train vs Valid vs Test RMSE')
plt.show()


#########################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error

# 生成數據
np.random.seed(42)
X = np.random.rand(1000, 10)
y = X[:, 0] * 10 + np.random.normal(0, 1, 1000)

# 分割數據集
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)

# 超參數搜索範圍
param_distributions = {
    'max_depth': [3, 4, 5, 6],         # 樹的深度
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # 學習率
    'n_estimators': [100, 200, 300, 500],     # 樹的數量
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的樣本比例
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # 每棵樹使用的特徵比例
    'reg_alpha': [0, 0.01, 0.1, 1],   # L1 正則化
    'reg_lambda': [1, 5, 10, 20]      # L2 正則化
}

# 初始化 XGBoost 模型
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# 使用 RandomizedSearchCV 進行隨機搜索
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_distributions,
    scoring='neg_mean_squared_error',
    n_iter=50,  # 隨機搜索迭代次數
    cv=3,       # 3 折交叉驗證
    random_state=42,
    verbose=1,
    n_jobs=-1   # 使用所有 CPU 加速計算
)
random_search.fit(X_train, y_train)

# 獲取最佳參數
print("Best parameters found by Random Search:")
best_params = random_search.best_params_
print(best_params)

# 使用 XGBoost 的低階 API 進行訓練並開啟 Early Stopping
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

best_params.update({
    "objective": "reg:squarederror",
    "eval_metric": "rmse",  # 評估指標為 RMSE
})

# 創建 evals_result 字典用於存儲結果
evals_result = {}

xgb_model_final = xgb.train(
    params=best_params,
    dtrain=dtrain,
    num_boost_round=500,
    evals=[(dtrain, 'train'), (dvalid, 'validate')],
    early_stopping_rounds=50,  # 如果驗證集 50 輪無提升則停止
    evals_result=evals_result,  # 存儲評估過程的結果
    verbose_eval=10
)

# 可視化 Early Stopping 過程
train_rmse = evals_result['train']['rmse']
validate_rmse = evals_result['validate']['rmse']
epochs = len(train_rmse)
x_axis = range(0, epochs)

plt.figure(figsize=(10, 6))
plt.plot(x_axis, train_rmse, label='Train RMSE', color='blue', linewidth=2)
plt.plot(x_axis, validate_rmse, label='Validate RMSE', color='orange', linewidth=2)
plt.axvline(x=xgb_model_final.best_iteration, color='red', linestyle='--', label='Best Iteration')
plt.title('Early Stopping: Train and Validate RMSE')
plt.xlabel('Boosting Iterations')
plt.ylabel('RMSE')
plt.legend()
plt.grid()
plt.show()

# 預測
y_train_pred = xgb_model_final.predict(dtrain)
y_valid_pred = xgb_model_final.predict(dvalid)
y_test_pred = xgb_model_final.predict(dtest)

# 評估模型
train_r2 = r2_score(y_train, y_train_pred)
valid_r2 = r2_score(y_valid, y_valid_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("\nModel Performance:")
print(f"Train R²: {train_r2:.4f} | Validate R²: {valid_r2:.4f} | Test R²: {test_r2:.4f}")
print(f"Train RMSE: {train_rmse:.4f} | Validate RMSE: {valid_rmse:.4f} | Test RMSE: {test_rmse:.4f}")

8. 正則化對泛化能力影響的測試
方式：
觀察不同的正則化參數對模型性能的影響，例如：

L2 正則化（Ridge Regression）
減少模型權重的幅度，防止模型過擬合。
L1 正則化（Lasso Regression）
強制某些權重為零，進行特徵選擇，減少模型複雜度。
通過正則化的方法可以有效提升泛化能力，但過強的正則化可能導致欠擬合。
