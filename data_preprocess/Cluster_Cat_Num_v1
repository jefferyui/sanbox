import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import community as community_louvain
import matplotlib.pyplot as plt

# 範例資料
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})

num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化，轉為 (n_samples,) 向量
scaler = StandardScaler()
num_vecs = [scaler.fit_transform(df[[col]]).flatten() for col in num_cols]

# 類別特徵 one-hot 後，對每個欄位取平均（得到向量）
ohe = OneHotEncoder(sparse_output=False)
cat_vecs = []
for col in cat_cols:
    oh = ohe.fit_transform(df[[col]])  # shape (n_samples, n_unique_cat)
    cat_vecs.append(oh.mean(axis=0))  # 轉成 1 維向量表示整個欄位分布

# 合併特徵向量 (list of vectors, 每個向量代表一個欄位)
feature_vectors = num_vecs + cat_vecs
feature_names = num_cols + cat_cols

# 將向量做成矩陣 (n_features, feature_dim)
# 注意：每個向量維度可能不同，補零對齊或用 padding，這裡示範簡單做法 — 用 numpy object array
max_len = max(vec.shape[0] for vec in feature_vectors)
mat = np.zeros((len(feature_vectors), max_len))

for i, vec in enumerate(feature_vectors):
    mat[i, :len(vec)] = vec

# 計算欄位間 cosine similarity（shape: n_features x n_features）
sim_matrix = cosine_similarity(mat)

# 建圖並 Louvain 分群
G = nx.Graph()
for i in range(len(feature_names)):
    for j in range(i+1, len(feature_names)):
        if sim_matrix[i,j] > 0.3:  # 閾值可調整
            G.add_edge(feature_names[i], feature_names[j], weight=sim_matrix[i,j])

partition = community_louvain.best_partition(G, weight='weight')

print("欄位分群結果:")
for feat, grp in partition.items():
    print(f"{feat}: 群組 {grp}")

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()




#########################
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import community as community_louvain
import matplotlib.pyplot as plt

# 範例資料
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})

num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化，轉為 (n_samples,) 向量
scaler = StandardScaler()
num_vecs = [scaler.fit_transform(df[[col]]).flatten() for col in num_cols]

# 類別特徵 OneHotEncoder with sparse_output=True
ohe = OneHotEncoder(sparse_output=True)
cat_vecs = []
for col in cat_cols:
    oh_sparse = ohe.fit_transform(df[[col]])              # 返回稀疏矩陣
    oh = oh_sparse.toarray()                              # 轉為密集陣列
    cat_vecs.append(oh.mean(axis=0))                      # 每欄的平均向量表示整體分布

# 合併特徵向量 (每個欄位為一個向量)
feature_vectors = num_vecs + cat_vecs
feature_names = num_cols + cat_cols

# 將向量做成等長矩陣 (padding)
max_len = max(vec.shape[0] for vec in feature_vectors)
mat = np.zeros((len(feature_vectors), max_len))
for i, vec in enumerate(feature_vectors):
    mat[i, :len(vec)] = vec

# 特徵相似度
sim_matrix = cosine_similarity(mat)

# 建圖並 Louvain 分群
G = nx.Graph()
for i in range(len(feature_names)):
    for j in range(i + 1, len(feature_names)):
        if sim_matrix[i, j] > 0.3:  # 可調整 threshold
            G.add_edge(feature_names[i], feature_names[j], weight=sim_matrix[i, j])

partition = community_louvain.best_partition(G, weight='weight')

# 分群輸出
print("欄位分群結果:")
for feat, grp in partition.items():
    print(f"{feat}: 群組 {grp}")

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True,
        node_color=[partition[n] for n in G.nodes()],
        cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()


####################
import numpy as np
import pandas as pd
import torch
from torch import nn
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from community import community_louvain  # pip install python-louvain

# --------------------------
# Step 1: 模擬一個複雜的資料集
# --------------------------
np.random.seed(42)
n_samples = 100

# df = pd.DataFrame({
#     'age': np.random.randint(20, 60, size=n_samples),
#     'income': np.random.randint(30000, 120000, size=n_samples),
#     'gender': np.random.choice(['M', 'F'], size=n_samples),
#     'city': np.random.choice(['Taipei', 'Kaohsiung', 'Taichung'], size=n_samples),
#     'ts_t1': np.random.randn(n_samples),
#     'ts_t2': np.random.randn(n_samples),
#     'ts_t3': np.random.randn(n_samples),
#     'ts_t4': np.random.randn(n_samples),
# })
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})
num_cols = ['age', 'income']
cat_cols = ['gender', 'education']
# ts_cols = ['ts_t1', 'ts_t2', 'ts_t3', 'ts_t4']

# --------------------------
# Step 2: 數值特徵標準化
# --------------------------
scaler = StandardScaler()
num_features = torch.tensor(scaler.fit_transform(df[num_cols]), dtype=torch.float32).T  # [n_cols x n_samples]

# --------------------------
# Step 3: 類別特徵用 embedding 編碼
# --------------------------
embeddings = []
embedding_dim = 4
for col in cat_cols:
    le = LabelEncoder()
    df[col + '_idx'] = le.fit_transform(df[col])
    n_categories = df[col + '_idx'].nunique()

    emb = nn.Embedding(num_embeddings=n_categories, embedding_dim=embedding_dim)
    emb_weights = emb(torch.tensor(df[col + '_idx'].values, dtype=torch.long))
    embeddings.append(emb_weights.T)

cat_features = torch.cat(embeddings, dim=0)  # [cat_cols_total_dim x n_samples]

# --------------------------
# Step 4: 時間序列特徵標準化
# --------------------------
# ts_features = torch.tensor(scaler.fit_transform(df[ts_cols]), dtype=torch.float32).T

# --------------------------
# Step 5: 合併所有欄位向量 (每欄位為一個向量)
# --------------------------
# feature_vectors = torch.cat([num_features, cat_features, ts_features], dim=0)  # [total_features x n_samples]
# feature_names = num_cols + cat_cols + ts_cols
feature_vectors = torch.cat([num_features, cat_features], dim=0)  # [total_features x n_samples]
feature_names = num_cols + cat_cols
# --------------------------
# Step 6: 計算特徵間 cosine similarity
# --------------------------
sim_matrix = cosine_similarity(feature_vectors.detach().numpy())

# --------------------------
# Step 7: 建立 graph 並用 Louvain 分群
# --------------------------
threshold = 0.3  # similarity threshold
G = nx.Graph()

for i, name_i in enumerate(feature_names):
    G.add_node(name_i)

for i in range(len(feature_names)):
    for j in range(i + 1, len(feature_names)):
        sim = sim_matrix[i, j]
        if sim > threshold:
            G.add_edge(feature_names[i], feature_names[j], weight=sim)

# --------------------------
# Step 8: Louvain 分群
# --------------------------
partition = community_louvain.best_partition(G, weight='weight')

# 分群結果輸出
cluster_df = pd.DataFrame({
    'feature': list(partition.keys()),
    'group': list(partition.values())
}).sort_values(by='group')

print(cluster_df)
# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()


#############################################
編碼方法	適用場景	優點	缺點
Label Encoding	有序類別或樹模型（如隨機森林）	簡單、高效	對所有非樹模型可能引入順序性問題，造成模型誤解
One-Hot Encoding	類別數少（<10），無序類別	無順序性問題，直觀使用	類別數多時可能導致維度爆炸
Binary Encoding	中型類別數據	降低維度，對類別數多的數據友好	不直觀，非線性模型效果明顯，但不適合線性模型
Target Encoding	有目標變量，類別型變量與目標有關聯	保留目標變量的統計特徵，適用於監督學習	容易導致過擬合，需搭配交叉驗證
Frequency Encoding	類別數多，目標變量無關	簡單高效，不會產生維度爆炸	可能引入數據分布偏差
Hash Encoding	高維類別數據（類別數量非常多，如城市、國家）	高效、固定特徵維度	哈希碰撞可能損失信息
Embedding Encoding	深度學習，類別數多	稠密表示，適合深度學習的輸入	訓練成本高，不適合簡單場景
Ordinal Encoding	有明確順序的類別型數據（如教育水平、等級）	保留類別的順序特徵	無法處理無序類別



#########################################
import numpy as np
import pandas as pd
import torch
from torch import nn
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from community import community_louvain  # pip install python-louvain

# --------------------------
# Step 1: 模擬一個複雜的資料集
# --------------------------
np.random.seed(42)
n_samples = 100

# df = pd.DataFrame({
#     'age': np.random.randint(20, 60, size=n_samples),
#     'income': np.random.randint(30000, 120000, size=n_samples),
#     'gender': np.random.choice(['M', 'F'], size=n_samples),
#     'city': np.random.choice(['Taipei', 'Kaohsiung', 'Taichung'], size=n_samples),
#     'ts_t1': np.random.randn(n_samples),
#     'ts_t2': np.random.randn(n_samples),
#     'ts_t3': np.random.randn(n_samples),
#     'ts_t4': np.random.randn(n_samples),
# })
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})
num_cols = ['age', 'income']
cat_cols = ['gender', 'education']
# ts_cols = ['ts_t1', 'ts_t2', 'ts_t3', 'ts_t4']

# --------------------------
# Step 2: 數值特徵標準化
# --------------------------
scaler = StandardScaler()
num_features = torch.tensor(scaler.fit_transform(df[num_cols]), dtype=torch.float32).T  # [n_cols x n_samples]

# --------------------------
# Step 3: 類別特徵用 embedding 編碼
# --------------------------
embeddings = []
embedding_dim = 8
for col in cat_cols:
    le = LabelEncoder()
    df[col + '_idx'] = le.fit_transform(df[col])
    n_categories = df[col + '_idx'].nunique()

    emb = nn.Embedding(num_embeddings=n_categories, embedding_dim=embedding_dim)
    emb_weights = emb(torch.tensor(df[col + '_idx'].values, dtype=torch.long))
    embeddings.append(emb_weights.T)

cat_features = torch.cat(embeddings, dim=0)  # [cat_cols_total_dim x n_samples]

# --------------------------
# Step 4: 時間序列特徵標準化
# --------------------------
# ts_features = torch.tensor(scaler.fit_transform(df[ts_cols]), dtype=torch.float32).T

# --------------------------
# Step 5: 合併所有欄位向量 (每欄位為一個向量)
# --------------------------
# feature_vectors = torch.cat([num_features, cat_features, ts_features], dim=0)  # [total_features x n_samples]
# feature_names = num_cols + cat_cols + ts_cols
feature_vectors = torch.cat([num_features, cat_features], dim=0)  # [total_features x n_samples]
feature_names = num_cols + cat_cols
# --------------------------
# Step 6: 計算特徵間 cosine similarity
# --------------------------
sim_matrix = cosine_similarity(feature_vectors.detach().numpy())

# --------------------------
# Step 7: 建立 graph 並用 Louvain 分群
# --------------------------
threshold = 0.7  # similarity threshold
G = nx.Graph()

for i, name_i in enumerate(feature_names):
    G.add_node(name_i)

for i in range(len(feature_names)):
    for j in range(i + 1, len(feature_names)):
        sim = sim_matrix[i, j]
        if sim > threshold:
            G.add_edge(feature_names[i], feature_names[j], weight=sim)

# --------------------------
# Step 8: Louvain 分群
# --------------------------
partition = community_louvain.best_partition(G, weight='weight')

# 分群結果輸出
cluster_df = pd.DataFrame({
    'feature': list(partition.keys()),
    'group': list(partition.values())
}).sort_values(by='group')

print(cluster_df)
# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()

#####################################################
import numpy as np
import pandas as pd
import torch
from torch import nn
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from community import community_louvain
import matplotlib.pyplot as plt

# 設置隨機種子
np.random.seed(42)
torch.manual_seed(42)

# 模擬數據
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})
num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化
scaler = StandardScaler()
num_features = torch.tensor(scaler.fit_transform(df[num_cols]), dtype=torch.float32).T

# 類別特徵用 embedding 編碼
embeddings = []
embedding_dim = 4  # 降低 embedding 維度
for col in cat_cols:
    le = LabelEncoder()
    df[col + '_idx'] = le.fit_transform(df[col])
    n_categories = df[col + '_idx'].nunique()

    emb = nn.Embedding(num_embeddings=n_categories, embedding_dim=embedding_dim)
    emb_weights = emb(torch.tensor(df[col + '_idx'].values, dtype=torch.long))
    embeddings.append(emb_weights.T)

cat_features = torch.cat(embeddings, dim=0)

# 合併所有特徵
feature_vectors = torch.cat([num_features, cat_features], dim=0)  # [total_features x n_samples]
feature_names = num_cols + cat_cols

# 計算特徵間 cosine similarity
sim_matrix = cosine_similarity(feature_vectors.detach().numpy())

# 建立 graph 並用 Louvain 分群
threshold = 0.8
G = nx.Graph()

for i, name_i in enumerate(feature_names):
    G.add_node(name_i)

for i in range(len(feature_names)):
    for j in range(i + 1, len(feature_names)):
        sim = sim_matrix[i, j]
        if sim > threshold:
            G.add_edge(feature_names[i], feature_names[j], weight=sim)

# Louvain 分群
partition = community_louvain.best_partition(G, weight='weight')

# 分群結果輸出
cluster_df = pd.DataFrame({
    'feature': list(partition.keys()),
    'group': list(partition.values())
}).sort_values(by='group')

print(cluster_df)

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()
# 調整後的穩定性改良
# 設置隨機種子（torch.manual_seed 和 seed=42）以減少隨機影響。
# 降低類別嵌入的維度（embedding_dim=4）。
# 增強特徵範圍標準化的一致性。
# 如果以上方法仍無法完全穩定結果，可以考慮進一步檢查數據集的多樣性，或對分群模型進行多次運行後取平均結果。
#######################################
https://blog.csdn.net/m0_49963403/article/details/140648244

##############3
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import community as community_louvain
import matplotlib.pyplot as plt

# 範例資料
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})

num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化，轉為 (n_samples,) 向量
scaler = StandardScaler()
num_vecs = [scaler.fit_transform(df[[col]]).flatten() for col in num_cols]

# # 類別特徵 one-hot 後，對每個欄位取平均（得到向量）
# ohe = OneHotEncoder(sparse_output=False)
# cat_vecs = []
# for col in cat_cols:
#     oh = ohe.fit_transform(df[[col]])  # shape (n_samples, n_unique_cat)
#     cat_vecs.append(oh.mean(axis=0))  # 轉成 1 維向量表示整個欄位分布

    # oh.mean(axis=0)
    # 這是取每一類出現的頻率（在整個欄位中）。

cat_vecs = []

for col in cat_cols:
    freq = df[col].value_counts(normalize=True)  # normalize=True → 得到比例
    encoded_col = df[col].map(freq)
    cat_vecs.append(encoded_col.values)



# 合併特徵向量 (list of vectors, 每個向量代表一個欄位)
feature_vectors = num_vecs + cat_vecs
feature_names = num_cols + cat_cols

# 將向量做成矩陣 (n_features, feature_dim)
# 注意：每個向量維度可能不同，補零對齊或用 padding，這裡示範簡單做法 — 用 numpy object array
max_len = max(vec.shape[0] for vec in feature_vectors)
mat = np.zeros((len(feature_vectors), max_len))

for i, vec in enumerate(feature_vectors):
    mat[i, :len(vec)] = vec

# 計算欄位間 cosine similarity（shape: n_features x n_features）
sim_matrix = cosine_similarity(mat)

# 建圖並 Louvain 分群
G = nx.Graph()
for i in range(len(feature_names)):
    for j in range(i+1, len(feature_names)):
        if sim_matrix[i,j] > 0.3:  # 閾值可調整
            G.add_edge(feature_names[i], feature_names[j], weight=sim_matrix[i,j])

partition = community_louvain.best_partition(G, weight='weight')

print("欄位分群結果:")
for feat, grp in partition.items():
    print(f"{feat}: 群組 {grp}")

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()



####################
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import community as community_louvain
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import random

# 固定種子以保證 reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# 範例資料
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})

num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化，轉為 (n_samples,) 向量
scaler = StandardScaler()
num_vecs = [scaler.fit_transform(df[[col]]).flatten() for col in num_cols]

# AutoEncoder 類別欄位壓縮器
class SimpleAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(SimpleAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

# 類別欄位轉向量
latent_dim = 4
cat_vecs = []

for col in cat_cols:
    # One-Hot encoding
    # ohe = OneHotEncoder(sparse_output=False)
    # X = ohe.fit_transform(df[[col]])
    # X_tensor = torch.tensor(X, dtype=torch.float32)


    # One-Hot encoding
    ohe = OneHotEncoder(sparse_output=True)  # 將 False 改成 True
    X = ohe.fit_transform(df[[col]])
    X_tensor = torch.tensor(X.toarray(), dtype=torch.float32)  # 使用 toarray() 方法轉換為稠密型

    # 建立 AutoEncoder
    ae = SimpleAE(input_dim=X.shape[1], latent_dim=latent_dim)
    optimizer = optim.Adam(ae.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()

    # 簡單訓練 autoencoder
    for epoch in range(200):  # 小資料快收斂
        optimizer.zero_grad()
        x_hat, _ = ae(X_tensor)
        loss = loss_fn(x_hat, X_tensor)
        loss.backward()
        optimizer.step()

    # 壓縮後的 bottleneck 表示（z）
    with torch.no_grad():
        _, z = ae(X_tensor)
        vec = z.mean(dim=0).numpy()  # 每個欄位的向量：平均所有 row

    cat_vecs.append(vec)

# 合併特徵向量 (list of vectors, 每個向量代表一個欄位)
feature_vectors = num_vecs + cat_vecs
feature_names = num_cols + cat_cols

# 將向量做成矩陣 (n_features, feature_dim)，補零對齊
max_len = max(vec.shape[0] for vec in feature_vectors)
mat = np.zeros((len(feature_vectors), max_len))

for i, vec in enumerate(feature_vectors):
    mat[i, :len(vec)] = vec

# 計算 cosine similarity（欄位間）
sim_matrix = cosine_similarity(mat)

# 建圖並 Louvain 分群
G = nx.Graph()
for i in range(len(feature_names)):
    for j in range(i+1, len(feature_names)):
        if sim_matrix[i, j] > 0.3:
            G.add_edge(feature_names[i], feature_names[j], weight=sim_matrix[i, j])

partition = community_louvain.best_partition(G, weight='weight')

print("欄位分群結果:")
for feat, grp in partition.items():
    print(f"{feat}: 群組 {grp}")

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()
########################################################
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import community as community_louvain
import matplotlib.pyplot as plt
import random

# 固定隨機種子
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# 範例資料
df = pd.DataFrame({
    'age': [25, 45, 35, 60],
    'income': [30000, 50000, 40000, 70000],
    'gender': ['M', 'F', 'M', 'F'],
    'education': ['High School', 'PhD', 'Master', 'Bachelor']
})

num_cols = ['age', 'income']
cat_cols = ['gender', 'education']

# 數值特徵標準化
scaler = StandardScaler()
num_vecs = [scaler.fit_transform(df[[col]]).flatten() for col in num_cols]

# Label Encoding 類別欄位
label_encoders = {}
encoded_cols = []
for col in cat_cols:
    le = LabelEncoder()
    encoded = le.fit_transform(df[col])
    label_encoders[col] = le
    encoded_cols.append(encoded)

# 定義 Embedding + AutoEncoder 模型
class EmbedAE(nn.Module):
    def __init__(self, num_categories, embed_dim=8, latent_dim=4):
        super(EmbedAE, self).__init__()
        self.embedding = nn.Embedding(num_categories, embed_dim)
        self.encoder = nn.Sequential(
            nn.Linear(embed_dim, latent_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, embed_dim),
            nn.Sigmoid()
        )
    def forward(self, x):
        x = self.embedding(x)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z

embedding_dim = 8
latent_dim = 4
cat_vecs = []

for i, col in enumerate(cat_cols):
    x = torch.tensor(encoded_cols[i], dtype=torch.long)

    model = EmbedAE(num_categories=len(label_encoders[col].classes_), embed_dim=embedding_dim, latent_dim=latent_dim)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()

    # 訓練 AutoEncoder
    for epoch in range(300):
        optimizer.zero_grad()
        x_hat, z = model(x)
        loss = loss_fn(x_hat, model.embedding(x))
        loss.backward()
        optimizer.step()

    with torch.no_grad():
        _, z = model(x)
        vec = z.mean(dim=0).numpy()
        cat_vecs.append(vec)

# 合併所有特徵向量
feature_vectors = num_vecs + cat_vecs
feature_names = num_cols + cat_cols

max_len = max(vec.shape[0] for vec in feature_vectors)
mat = np.zeros((len(feature_vectors), max_len))
for i, vec in enumerate(feature_vectors):
    mat[i, :len(vec)] = vec

sim_matrix = cosine_similarity(mat)

# Louvain 分群
G = nx.Graph()
for i in range(len(feature_names)):
    for j in range(i+1, len(feature_names)):
        if sim_matrix[i,j] > 0.3:
            G.add_edge(feature_names[i], feature_names[j], weight=sim_matrix[i,j])

partition = community_louvain.best_partition(G, weight='weight')

print("欄位分群結果:")
for feat, grp in partition.items():
    print(f"{feat}: 群組 {grp}")

# 畫圖
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=True, node_color=[partition.get(n) for n in G.nodes()], cmap=plt.cm.Set3)
plt.title("Feature Clustering")
plt.show()

這個方法的優點：
Embedding 可捕捉類別間相似性（非序數）

AutoEncoder 壓縮學習去除冗餘、提取主要特徵

Bottleneck 輸出向量穩定且更具代表性

可以用相似度分群，效果比純 Label Encoding 好很多

