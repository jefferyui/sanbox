import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

# 加载 Titanic 数据集
data = sns.load_dataset('titanic')

# 数据预处理
data = data.dropna(subset=['age', 'embarked', 'sex', 'class', 'fare'])  # 删除缺失值

# 将性别列编码为数值
data['sex'] = data['sex'].map({'male': 0, 'female': 1})

# 将 'class' 列转换为 object 类型，以便 LabelEncoder 能处理
data['class'] = data['class'].astype('object')

# 使用 LabelEncoder 对 'embarked' 和 'class' 列进行编码
label_encoder = LabelEncoder()
data['embarked'] = label_encoder.fit_transform(data['embarked'].fillna('missing'))  # 填充缺失值并编码
data['class'] = label_encoder.fit_transform(data['class'].fillna('missing'))  # 填充缺失值并编码

# 目标列提取
target = data['survived']
data = data.drop(columns=['survived'])  # 删除目标列，只保留特征

# 确保所有列都是数值型
data = data.select_dtypes(include=['number'])

# 特征和标签
X = data.values
y = target.values

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 切分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换为 PyTorch 张量
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

# 定义 Transformer 分类模型
class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim, d_model=64, nhead=4, num_layers=2):
        super(TransformerModel, self).__init__()
        
        self.embedding = nn.Linear(input_dim, d_model)
        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, d_model))  # 最大序列长度假设为1000
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers)
        self.fc = nn.Linear(d_model, output_dim)
    
    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(0)  # 增加batch维度
        x = x + self.positional_encoding[:, :x.size(1), :]
        x = self.transformer(x, x)
        x = x.squeeze(0)  # 去除batch维度
        x = self.fc(x)
        return x

# 初始化模型、损失函数和优化器
model = TransformerModel(input_dim=X_train.shape[1], output_dim=2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 2 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 评估模型
model.eval()
with torch.no_grad():
    y_pred = model(X_test)
    _, predicted = torch.max(y_pred, 1)
    accuracy = accuracy_score(y_test, predicted)
    print(f'Accuracy: {accuracy * 100:.2f}%')
