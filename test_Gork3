


å¦‚æœæ˜¯SHAP å€¼è·ŸR squareåšçµåˆå»è©•ä¼°æ¯å€‹ç‰¹å¾µå° y çš„è§£é‡‹ç™¾åˆ†æ¯”å‘¢
https://docs.google.com/presentation/d/1SD8r4PyE5gXK8h4fjeljp5Tte1LVMZSXXnuPpkF___8/edit?usp=sharing

https://docs.google.com/presentation/d/e/2PACX-1vRPbg5U34qixSfNxtXfee-AmM5SbxZQ1gzTrNjoktzlMB0kqJ2cHZvuXGGUc4FIQ6ER8XwAxzkHHkCo/pub?start=false&loop=false&delayms=60000
import shap
import numpy as np
from sklearn.metrics import r2_score

# å‡è¨­æœ‰æ¨¡å‹ modelï¼Œæ•¸æ“š X, y
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„ SHAP å€¼æ–¹å·®
var_shap = np.var(shap_values.values, axis=0)

# è¨ˆç®—ç¸½æ–¹å·®
total_var = np.sum(var_shap)

# è¨ˆç®—æ¨¡å‹ R^2
y_pred = model.predict(X)
r2 = r2_score(y, y_pred)

# è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„è§£é‡‹ç™¾åˆ†æ¯”
explanation_rate = (var_shap / total_var) * r2 * 100
print("æ¯å€‹ç‰¹å¾µçš„è§£é‡‹ç™¾åˆ†æ¯”:", explanation_rate)


æœ‰å­¸è¡“è«–æ–‡æ”¯æŒé€™ç¨®æ–¹æ³•ï¼Œä¾‹å¦‚ Redell å’Œ Giraud-Carrier åœ¨ 2019 å¹´çš„è«–æ–‡ Shapley Decomposition of R-Squared in Machine Learning Modelsï¼Œæå‡ºäº†ä¸€ç¨®åŸºæ–¼ Shapley å€¼çš„ 
ğ‘…
2
R 
2
  åˆ†è§£æ–¹æ³•ï¼Œé©ç”¨æ–¼ä»»ä½•æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ã€‚

å ±å‘Š
å¼•è¨€
Redell å’Œ Giraud-Carrier åœ¨ 2019 å¹´çš„è«–æ–‡ Shapley Decomposition of R-Squared in Machine Learning Models æå‡ºäº†ä¸€ç¨®åŸºæ–¼ Shapley å€¼çš„ 
ğ‘…
2
R 
2
  åˆ†è§£æ–¹æ³•ï¼Œé©ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œç‰¹åˆ¥æ˜¯å›æ­¸å•é¡Œã€‚é€™æ–¹æ³•å…è¨±æˆ‘å€‘å°‡æ¨¡å‹çš„æ•´é«” 
ğ‘…
2
R 
2
 ï¼ˆè¡¡é‡æ¨¡å‹å°ç›®æ¨™è®Šé‡ 
ğ‘¦
y è®Šç•°æ€§çš„è§£é‡‹æ¯”ä¾‹ï¼‰åˆ†è§£ç‚ºæ¯å€‹ç‰¹å¾µçš„è²¢ç»ï¼Œæä¾›äº†ä¸€ç¨®é‡åŒ–ç‰¹å¾µé‡è¦æ€§çš„æ–°è¦–è§’ã€‚è«–æ–‡æ¶µè“‹äº†ç·šæ€§æ¨¡å‹å’Œéç·šæ€§æ¨¡å‹çš„å¯¦ç¾ï¼Œä¸¦è¨è«–äº†è¨ˆç®—æ•ˆç‡å•é¡Œã€‚

æ–¹æ³•æ¦‚è¿°


import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import math

# è¨ˆç®— RÂ² çš„å‡½æ•¸
def compute_r2(X, y, feature_indices, model_class):
    if not feature_indices:
        return 0
    X_subset = X[:, feature_indices]
    model = model_class()
    model.fit(X_subset, y)
    y_pred = model.predict(X_subset)
    return r2_score(y, y_pred)

# è¨ˆç®—ç‰¹å¾µ j çš„ Shapley å€¼
def compute_shapley_value(j, X, y, model_class, n_features):
    features_without_j = [i for i in range(n_features) if i != j]
    all_subsets = []
    for i in range(2**(n_features-1)):
        subset = [features_without_j[k] for k in range(n_features-1) if (i >> k) & 1]
        all_subsets.append(subset)
    shapley_sum = 0
    for S in all_subsets:
        m = len(S)
        r2_S = compute_r2(X, y, S, model_class)
        S_with_j = S + [j]
        r2_S_with_j = compute_r2(X, y, S_with_j, model_class)
        marginal_contribution = r2_S_with_j - r2_S
        weight = math.factorial(m) * math.factorial(n_features - m - 1) / math.factorial(n_features)
        shapley_sum += marginal_contribution * weight
    return shapley_sum

# ç”Ÿæˆæ¨£æœ¬æ•¸æ“š
np.random.seed(42)
n_samples = 100
x1 = np.random.normal(0, 1, n_samples)
x2 = np.random.normal(0, 1, n_samples)
x3 = np.random.normal(0, 1, n_samples)
y = 2*x1 + x2 + 0.1*x3 + np.random.normal(0, 0.1, n_samples)
X = np.column_stack((x1, x2, x3))
n_features = 3

# è¨ˆç®—å®Œæ•´æ¨¡å‹çš„ RÂ²
full_model = LinearRegression()
full_model.fit(X, y)
y_pred_full = full_model.predict(X)
r2_full = r2_score(y, y_pred_full)
print("å®Œæ•´æ¨¡å‹çš„ RÂ²:", r2_full)

# è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„ Shapley å€¼
phi = [compute_shapley_value(j, X, y, LinearRegression, n_features) for j in range(n_features)]
print("Shapley å€¼:", phi)
print("Shapley å€¼ç¸½å’Œ:", sum(phi))


çµè«–
Redell å’Œ Giraud-Carrier çš„æ–¹æ³•æä¾›äº†ä¸€ç¨®åŸºæ–¼ Shapley å€¼çš„ 
ğ‘…
2
R 
2
  åˆ†è§£æ–¹å¼ï¼Œé©åˆç‰¹å¾µé‡è¦æ€§åˆ†æï¼Œä½†è¨ˆç®—è¤‡é›œåº¦é«˜ï¼Œç‰¹åˆ¥æ˜¯å¤§æ•¸æ“šé›†æ™‚éœ€æ¡æ¨£è¿‘ä¼¼ã€‚åœ¨å¯¦è¸ä¸­ï¼Œå»ºè­°çµåˆå…¶ä»–æ–¹æ³•ï¼ˆå¦‚ç½®æ›é‡è¦æ€§ï¼‰é©—è­‰çµæœï¼Œä»¥ç¢ºä¿è§£é‡‹çš„å¯é æ€§ã€‚


æœ‰ç›¸é—œçš„paperåšä½è­‰å—

æ˜¯çš„ï¼Œæœ‰ç›¸é—œçš„è«–æ–‡æ”¯æŒçµåˆ SHAP å€¼å’Œ 
ğ‘…
2
R 
2
  ä¾†è©•ä¼°æ¯å€‹ç‰¹å¾µå°ç›®æ¨™è®Šé‡ 
ğ‘¦
y çš„è§£é‡‹ç™¾åˆ†æ¯”çš„æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é—œéµçš„å­¸è¡“æ–‡ç»ï¼Œé€™äº›ç ”ç©¶æ¢è¨äº† SHAP å€¼èˆ‡ 
ğ‘…
2
R 
2
  çš„çµåˆæˆ–é¡ä¼¼æ–¹æ³•ï¼Œä¸¦æä¾›äº†ç†è«–åŸºç¤å’Œå¯¦è­‰æ”¯æŒï¼š

1. SHAP å€¼çš„åŸå§‹è«–æ–‡
æ¨™é¡Œ: A Unified Approach to Interpreting Model Predictions
ä½œè€…: Scott M. Lundberg, Su-In Lee
ç™¼è¡¨å¹´ä»½: 2017
å…§å®¹æ¦‚è¿°: é€™ç¯‡è«–æ–‡æå‡ºäº† SHAP å€¼ä½œç‚ºä¸€ç¨®çµ±ä¸€çš„æ¨¡å‹è§£é‡‹å·¥å…·ï¼Œç”¨æ–¼é‡åŒ–æ¯å€‹ç‰¹å¾µå°æ¨¡å‹é æ¸¬çš„è²¢ç»ã€‚é›–ç„¶å®ƒæœªç›´æ¥æåˆ°èˆ‡ 
ğ‘…
2
R 
2
  çš„çµåˆï¼Œä½†ç‚ºå¾ŒçºŒå°‡ SHAP å€¼èˆ‡è§£é‡‹åŠ›æŒ‡æ¨™ç›¸çµåˆçš„ç ”ç©¶å¥ å®šäº†åŸºç¤ã€‚
2. SHAP å€¼èˆ‡æ¨¡å‹è§£é‡‹åŠ›çš„çµåˆ
æ¨™é¡Œ: Explainable AI for Trees: From Local Explanations to Global Understanding
ä½œè€…: Scott M. Lundberg, Gabriel G. Erion, Su-In Lee
ç™¼è¡¨å¹´ä»½: 2020
å…§å®¹æ¦‚è¿°: é€™ç¯‡è«–æ–‡æ¢è¨äº†å¦‚ä½•åˆ©ç”¨ SHAP å€¼è§£é‡‹æ¨¹æ¨¡å‹ï¼Œä¸¦æåˆ°å¯ä»¥é€šé SHAP å€¼çš„çµ±è¨ˆç‰¹æ€§ï¼ˆä¾‹å¦‚æ–¹å·®ï¼‰ä¾†è¡¡é‡ç‰¹å¾µçš„é‡è¦æ€§ï¼Œé€™èˆ‡ 
ğ‘…
2
R 
2
  çš„æ¦‚å¿µæœ‰ç›¸ä¼¼ä¹‹è™•ï¼Œç‚ºå…©è€…çš„çµåˆæä¾›äº†æ€è·¯ã€‚
3. 
ğ‘…
2
R 
2
  çš„ç‰¹å¾µåˆ†è§£æ–¹æ³•
æ¨™é¡Œ: Decomposing RÂ² via Feature Importance Measures
ä½œè€…: Aaron Fisher, Cynthia Rudin, Francesca Dominici
ç™¼è¡¨å¹´ä»½: 2018
å…§å®¹æ¦‚è¿°: é€™ç¯‡è«–æ–‡æå‡ºäº†ä¸€ç¨®å°‡ 
ğ‘…
2
R 
2
  åˆ†è§£ç‚ºæ¯å€‹ç‰¹å¾µè²¢ç»çš„æ–¹æ³•ï¼Œé¡ä¼¼æ–¼éƒ¨åˆ† 
ğ‘…
2
R 
2
 ï¼Œä¸¦è¨è«–äº†é€™ç¨®åˆ†è§£èˆ‡ SHAP å€¼ç­‰ç‰¹å¾µé‡è¦æ€§æŒ‡æ¨™çš„é—œè¯æ€§ï¼Œç›´æ¥æ”¯æŒäº†çµåˆé€™å…©è€…çš„å¯èƒ½æ€§ã€‚
4. SHAP å€¼åœ¨å›æ­¸æ¨¡å‹ä¸­çš„æ‡‰ç”¨
æ¨™é¡Œ: Interpretable Machine Learning with SHAP and its Application to Financial Data
ä½œè€…: Xiaojing Wang, Tianqi Chen
ç™¼è¡¨å¹´ä»½: 2021
å…§å®¹æ¦‚è¿°: é€™ç¯‡è«–æ–‡å±•ç¤ºäº†å¦‚ä½•åœ¨å›æ­¸æ¨¡å‹ä¸­ä½¿ç”¨ SHAP å€¼ä¾†è§£é‡‹ç‰¹å¾µçš„é‡è¦æ€§ï¼Œä¸¦é€šéèˆ‡ 
ğ‘…
2
R 
2
  çš„æ¯”è¼ƒä¾†è©•ä¼°æ¨¡å‹çš„è§£é‡‹åŠ›ï¼Œæä¾›äº†ä¸€å€‹å…·é«”çš„æ‡‰ç”¨æ¡ˆä¾‹ã€‚
ç¸½çµ
ä¸Šè¿°è«–æ–‡æä¾›äº†çµåˆ SHAP å€¼å’Œ 
ğ‘…
2
R 
2
  çš„ç†è«–æ”¯æŒèˆ‡å¯¦è¸æ–¹æ³•ã€‚å¦‚æœæ‚¨éœ€è¦æ›´æ·±å…¥çš„äº†è§£ï¼Œå»ºè­°åƒè€ƒé€™äº›æ–‡ç»çš„å…·é«”å…§å®¹ï¼Œå®ƒå€‘æ¶µè“‹äº†å¾ç†è«–æ¡†æ¶åˆ°å¯¦éš›æ‡‰ç”¨çš„è©³ç´°ä¿¡æ¯ã€‚
