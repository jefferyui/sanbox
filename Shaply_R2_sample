https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/winequality-white.csv
Shapley Decomposition of R-Squared in Machine
Learning Models
根據論文建立 python code範例
用xgboost重作

# do
import numpy as np
import pandas as pd
import xgboost as xgb
import shap


def calculate_shapley_r2(X, y, model):
    # 训练模型
    model.fit(X, y)

    # 计算Shapley值
    explainer = shap.Explainer(model)
    shap_values = explainer(X)

    # 计算基线R²
    y_pred = model.predict(X)
    var_y_pred = np.var(y_pred)
    var_res = np.var(y - y_pred)
    R_baseline_2 = var_y_pred / (var_y_pred + var_res)
    print("R_baseline_2:",R_baseline_2)
    R_shap_2_f = []
    for f in range(X.shape[1]):
        # 计算Shapley修正预测值
        y_pred_shap_f = y_pred - shap_values[:, f].values
        var_res_f = np.var(y - y_pred_shap_f)

        # 计算特征级R²
        numerator = R_baseline_2 - min(var_res / var_res_f, 1) * R_baseline_2
        denominator = np.sum([R_baseline_2 - min(var_res / np.var(y - (y_pred - shap_values[:, i].values)), 1) * R_baseline_2 for i in range(X.shape[1])])
        R_shap_2_f.append(numerator / denominator * R_baseline_2)

    return R_shap_2_f


# 示例数据，这里使用随机数据代替真实数据
# num_samples = 100
# num_features = 5
# X = pd.DataFrame(np.random.randn(num_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])
# y = pd.Series(np.random.randn(num_samples))
data = pd.read_csv('winequality-white.csv', sep=',')
X = data.drop('quality', axis=1)
y = data['quality']

# 使用XGBoost回归模型
model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)

# 计算特征级R²
R_shap_2_f_values = calculate_shapley_r2(X, y, model)

# 展示结果
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Shapley - R²': R_shap_2_f_values
})
print(feature_importance_df)
feature_importance_df['Shapley - R²'].sum()




##############
# Deepseek R1
import numpy as np
import pandas as pd
import shap
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import xgboost as xgb

# 示例数据加载（以葡萄酒质量数据集为例）
data = pd.read_csv('winequality-white.csv', sep=',')
X = data.drop('quality', axis=1)
y = data['quality']

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换数据格式为DMatrix（可选）
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# 训练XGBoost模型
params = {
    'objective': 'reg:squarederror',
    'n_estimators': 1000,
    'learning_rate': 0.1,
    'random_state': 42
}
model = xgb.train(params, dtrain, num_boost_round=1000)

# 计算基准预测和R²
y_pred = model.predict(dtest)
baseline_r2 = r2_score(y_test, y_pred)
print(f"Baseline R²: {baseline_r2:.4f}")

# 计算Shapley值
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 初始化存储特征重要性的字典
feature_importance = {col: 0 for col in X.columns}

# 计算总残差方差
residual_baseline = y_test - y_pred
var_res_baseline = np.var(residual_baseline)

# 遍历每个特征计算贡献
for f_idx, feature in enumerate(X.columns):
    # 生成Shapley调整后的预测
    y_pred_shap = y_pred - shap_values[:, f_idx]
    
    # 计算调整后的残差方差
    residual_shap = y_test - y_pred_shap
    var_res_shap = np.var(residual_shap)
    
    # 计算方差比例（公式6核心部分）
    ratio = min(var_res_baseline / var_res_shap, 1)
    delta_r2 = baseline_r2 - ratio * baseline_r2
    
    feature_importance[feature] = delta_r2

# 归一化处理（确保总和等于baseline R²）
total = sum(feature_importance.values())
for feature in feature_importance:
    feature_importance[feature] = (feature_importance[feature] / total) * baseline_r2

# 输出结果
print("\nFeature Importance (R² decomposition):")

sum_shap_R = 0
for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
    sum_shap_R +=importance
    print(f"{feature}: {importance:.4f}")
print("sum_shap_R",sum_shap_R)
# 计算σ_unique（公式7）
var_y = np.var(y_test)
sum_var_shap = sum([np.var(y_test - (y_pred - shap_values[:, f_idx])) for f_idx in range(X.shape[1])])
sigma_unique = sum_var_shap / (var_y - var_res_baseline)
print(f"\nσ_unique: {sigma_unique:.4f}")


###############

# 用xgboost重寫公式6跟7python 範例
import numpy as np
import pandas as pd
import xgboost as xgb
import shap


def calculate_feature_r2(X, y, model):
    """
    计算特征级别的 R^2（公式 6）
    """
    # 训练模型
    model.fit(X, y)
    # 计算 Shapley 值
    explainer = shap.Explainer(model)
    shap_values = explainer(X).values

    # 计算基线 R^2
    y_pred = model.predict(X)
    var_y_pred = np.var(y_pred)
    var_res = np.var(y - y_pred)
    R_baseline_2 = var_y_pred / (var_y_pred + var_res)

    num_features = X.shape[1]
    R_shap_2_f = []

    for f in range(num_features):
        # 计算移除当前特征影响后的预测值
        y_pred_shap_f = y_pred - shap_values[:, f]
        var_res_f = np.var(y - y_pred_shap_f)

        # 计算特征级别的 R^2
        numerator = R_baseline_2 - min(var_res / var_res_f, 1) * R_baseline_2
        denominator = sum([R_baseline_2 - min(var_res / np.var(y - (y_pred - shap_values[:, i])), 1) * R_baseline_2 for i in range(num_features)])
        R_shap_2_f.append(numerator / denominator * R_baseline_2)

    return R_shap_2_f


def calculate_sigma_unique(X, y, model):
    """
    计算可唯一归因于每个特征的方差比例（公式 7）
    """
    # 训练模型
    model.fit(X, y)
    # 计算 Shapley 值
    explainer = shap.Explainer(model)
    shap_values = explainer(X).values

    # 计算模型预测值
    y_pred = model.predict(X)

    # 计算分母部分：var(y - y_mean) - var(y - y_pred)
    var_y_mean = np.var(y - np.mean(y))
    var_y_res = np.var(y - y_pred)
    denominator = var_y_mean - var_y_res

    # 计算分子部分：sum(var(y - y_pred_shap_f))
    numerator = 0
    num_features = X.shape[1]
    for f in range(num_features):
        y_pred_shap_f = y_pred - shap_values[:, f]
        numerator += np.var(y - y_pred_shap_f)

    # 计算 sigma_unique
    sigma_unique = numerator / denominator if denominator != 0 else 0
    return sigma_unique


# 示例数据生成
# num_samples = 100
# num_features = 5
# X = pd.DataFrame(np.random.randn(num_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])
# y = pd.Series(np.random.randn(num_samples))
data = pd.read_csv('winequality-white.csv', sep=',')
X = data.drop('quality', axis=1)
y = data['quality']
# 使用 XGBoost 回归模型
model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)

# 计算特征级别的 R^2
feature_r2_values = calculate_feature_r2(X, y, model)
feature_r2_df = pd.DataFrame({
    'Feature': X.columns,
    'Feature R^2': feature_r2_values
})
print("特征级别的 R^2:")
print(feature_r2_df)

# 计算可唯一归因于每个特征的方差比例
sigma_unique = calculate_sigma_unique(X, y, model)
print(f"\n可唯一归因于每个特征的方差比例 (sigma_unique): {sigma_unique}")



#####################
import xgboost as xgb
import shap
import numpy as np
import pandas as pd
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# # 1. 加载数据集
# data = load_boston()
# X = pd.DataFrame(data.data, columns=data.feature_names)
# y = data.target

# 2. 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. 训练XGBoost模型
model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# 4. 计算基线R²
y_pred = model.predict(X_test)
var_y_pred = np.var(y_pred)
residuals = y_test - y_pred
var_res = np.var(residuals)
R_baseline = var_y_pred / (var_y_pred + var_res)
print(f"Baseline R²: {R_baseline:.4f}")

# 5. 计算Shapley值
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)  # 形状为 (样本数, 特征数)

# 6. 计算每个特征的R²贡献
feature_importance = {}
n_features = X_test.shape[1]

for f in range(n_features):
    # 计算修改后的预测值
    y_pred_shap = y_pred - shap_values[:, f]
    
    # 计算新的残差方差
    new_residuals = y_test - y_pred_shap
    var_res_f = np.var(new_residuals)
    
    # 处理数值不稳定情况
    if var_res_f == 0:
        ratio = 1.0
    else:
        ratio = var_res / var_res_f
        ratio = min(ratio, 1.0)  # 确保不超过1
    
    # 计算贡献
    contribution = R_baseline * (1 - ratio)
    feature_importance[X.columns[f]] = contribution

# 7. 归一化并计算最终R²
total_contribution = sum(feature_importance.values())
if total_contribution > 0:
    for feature in feature_importance:
        feature_importance[feature] *= R_baseline / total_contribution

# 8. 输出结果
sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
print("\nFeature Importance (R² Decomposition):")
for feature, importance in sorted_features:
    print(f"{feature}: {importance:.4f}")

# 可视化示例（需要安装matplotlib）
import matplotlib.pyplot as plt

features = [f[0] for f in sorted_features]
importances = [f[1] for f in sorted_features]

plt.figure(figsize=(10, 6))
plt.barh(features, importances)
plt.xlabel('Explained Variance (R²)')
plt.title('Feature Importance Based on Shapley Decomposition')
plt.gca().invert_yaxis()
plt.show()
